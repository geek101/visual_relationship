{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images Visual Relationship project\n",
    "This project is based on the [Google AI Open Images - Visual Relationship Track Kaggle Challenge](https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track).\n",
    "\n",
    "The challenge is to build the best performing algorithm for automatically detecting relationships triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using COCO trained weights: resnet50_coco_best_v2.1.0.h5 from path: /home/powell/work/scpd/vision_project/visual_relationship/pretrained_models/resnet50_coco_best_v2.1.0.h5, downloaded from: https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5\n",
      "Keras version: 2.2.4\n",
      "Tensorflow version: 1.4.0\n",
      "Tensorflow lib config: /home/powell/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import wget\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import collections\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import parallel \n",
    "import concurrent.futures\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import __version__ as keras_version\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Lambda, Cropping2D, Reshape, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import concatenate\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# https://github.com/fizyr/keras-retinanet/blob/master/examples/ResNet50RetinaNet.ipynb\n",
    "import keras_retinanet\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image, compute_resize_scale\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "#def get_session():\n",
    "#    config = tf.ConfigProto()\n",
    "#    config.gpu_options.allow_growth = True\n",
    "#    return tf.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "#keras.backend.tensorflow_backend.set_session(get_session())\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5\n",
    "RCNN_COCO_MODEL_URL = \"https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5\"\n",
    "RCNN_COCO_MODEL = \"resnet50_coco_best_v2.1.0.h5\"\n",
    "\n",
    "# Local path to trained weights file\n",
    "MODEL_PATH = os.path.join(os.path.join(os.getcwd(), \"pretrained_models\"), RCNN_COCO_MODEL)\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print ('Downloading COCO trained weights: {} to path: {}'.format(RCNN_COCO_MODEL, MODEL_PATH))  \n",
    "    wget.download(RCNN_COCO_MODEL_URL, MODEL_PATH)  \n",
    "else:\n",
    "    print ('Using COCO trained weights: {} from path: {}, downloaded from: {}'.format(\n",
    "        RCNN_COCO_MODEL, MODEL_PATH, RCNN_COCO_MODEL_URL))\n",
    "    \n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "print ('Keras version: {}'.format(keras_version))\n",
    "print ('Tensorflow version: {}'.format(tf.__version__))\n",
    "print ('Tensorflow lib config: {}'.format(tf.sysconfig.get_lib()))\n",
    "\n",
    "from helpers import get_fid, process_labels_from_csv_input, process_raw_csv_input, get_data_dir_from_raw_single_dir \n",
    "from helpers import get_data_from_dir_recursive, bounding_box_to_plt, two_bounding_boxes_to_plt, show_images\n",
    "from helpers import show_given_images, show_random_images, _resize_job_helper, chunks, resize_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels loaded:61\n",
      "0: /m/078n6m-Coffee table\n",
      "1: /m/01mzpv-Chair\n",
      "2: /m/01226z-Football\n",
      "3: /m/0h2r6-Van\n",
      "4: /m/0pg52-Taxi\n",
      "5: /m/0cvnqh-Bench\n",
      "6: /m/0hg7b-Microphone\n",
      "7: /m/0dnr7-Textile\n",
      "8: /m/080hkjn-Handbag\n",
      "9: /m/01f91_-Pretzel\n",
      "10: /m/0k4j-Car\n",
      "11: /m/078jl-Snake\n",
      "12: /m/04_sv-Motorcycle\n",
      "13: /m/0bwd_0j-Elephant\n",
      "14: /m/02hj4-Dolphin\n",
      "15: /m/029bxz-Oven\n",
      "16: /m/019w40-Surfboard\n",
      "17: /m/01s55n-Suitcase\n",
      "18: /m/05r655-Girl\n",
      "19: /m/01yrx-Cat\n",
      "20: /m/05z87-Plastic\n",
      "21: /m/03k3r-Horse\n",
      "22: /m/050k8-Mobile phone\n",
      "23: /m/0fx9l-Microwave oven\n",
      "24: /m/0199g-Bicycle\n",
      "25: /m/0h8my_4-Tennis racket\n",
      "26: /m/04bcr3-Table\n",
      "27: /m/07y_7-Violin\n",
      "28: /m/04yx4-Man\n",
      "29: /m/0584n8-Briefcase\n",
      "30: /m/01599-Beer\n",
      "31: /m/09tvcd-Wine glass\n",
      "32: /m/0342h-Guitar\n",
      "33: /m/04dr76w-Bottle\n",
      "34: /m/0cmx8-Spoon\n",
      "35: /m/026t6-Drum\n",
      "36: /m/071p9-Ski\n",
      "37: /m/0bt9lr-Dog\n",
      "38: /m/08pbxl-Monkey\n",
      "39: /m/03bt1vf-Woman\n",
      "40: /m/01bl7v-Boy\n",
      "41: /m/0l14j_-Flute\n",
      "42: /m/04ctx-Knife\n",
      "43: /m/05r5c-Piano\n",
      "44: /m/0dv9c-Racket\n",
      "45: /m/01940j-Backpack\n",
      "46: /m/03ssj5-Bed\n",
      "47: /m/02jvh9-Mug\n",
      "48: /m/05ctyq-Tennis ball\n",
      "49: /m/01_5g-Chopsticks\n",
      "50: /m/03qrc-Hamster\n",
      "51: /m/06__v-Snowboard\n",
      "52: /m/04lbp-Leather\n",
      "53: /m/083vt-Wood\n",
      "54: /m/0dt3t-Fork\n",
      "55: /m/05_5p_0-Table tennis racket\n",
      "56: /m/0wdt60w-Rugby ball\n",
      "57: /m/03m3pdh-Sofa bed\n",
      "58: /m/0dv5r-Camera\n",
      "59: /m/02p5f1q-Coffee cup\n",
      "60: /m/01y9k5-Desk\n",
      "Number of unique labels from training data loaded: 61\n",
      "Number of unique labels from test data loaded: 59\n",
      "Total missing label count: 2\n",
      "/m/029bxz-Oven\n",
      "/m/0fx9l-Microwave oven\n"
     ]
    }
   ],
   "source": [
    "annotations_file='fullsize_train_annotations.csv'\n",
    "annotations_test_file='fullsize_test_annotations.csv'\n",
    "classes_file='fullsize_classes.csv'\n",
    "# Load the labels\n",
    "label_tag_to_name = process_labels_from_csv_input()\n",
    "def load_labels(label_tag_to_name, labels_file=classes_file):\n",
    "    labels_to_names = {}\n",
    "    # Process the labels info files and create a key value pair\n",
    "    with open(labels_file, encoding='utf8') as f:\n",
    "        rows = csv.reader(f)\n",
    "        for row in rows:\n",
    "            labels_to_names[int(row[1])]= (label_tag_to_name[row[0]], row[0])\n",
    "    return labels_to_names\n",
    "\n",
    "labels_to_names = load_labels(label_tag_to_name, labels_file=classes_file)\n",
    "#print(label_tag_to_name)\n",
    "print (\"Number of labels loaded:{}\".format(len(labels_to_names)))\n",
    "for k, v in sorted(labels_to_names.items()):\n",
    "    print (\"{}: {}-{}\".format(k, v[1], v[0]))\n",
    "\n",
    "def load_labels_and_dataset_from_data(label_tag_to_name, data_file=annotations_file):\n",
    "    labels_to_names = {}\n",
    "    # Process the labels info files and create a key value pair\n",
    "    with open(data_file, encoding='utf8') as f:\n",
    "        rows = csv.reader(f)\n",
    "        for row in rows:\n",
    "            \n",
    "            labels_to_names[row[-1]] = label_tag_to_name[row[-1]]\n",
    "    return labels_to_names\n",
    "\n",
    "train_labels_to_names = load_labels_and_dataset_from_data(label_tag_to_name, data_file=annotations_file)\n",
    "print (\"Number of unique labels from training data loaded: {}\".format(len(train_labels_to_names)))\n",
    "   \n",
    "test_labels_to_names = load_labels_and_dataset_from_data(label_tag_to_name, data_file=annotations_test_file)\n",
    "print (\"Number of unique labels from test data loaded: {}\".format(len(test_labels_to_names)))\n",
    "\n",
    "missing_label_set = set(train_labels_to_names.keys()) - set(test_labels_to_names.keys())\n",
    "print (\"Total missing label count: {}\".format(len(missing_label_set)))\n",
    "for l in missing_label_set:\n",
    "    print (\"{}-{}\".format(l, label_tag_to_name[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the inference data and prep it for next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training y data size: 87547\n",
      "Test y data size: 18012\n"
     ]
    }
   ],
   "source": [
    "infer_result_dir_prefix = 'fullsize_infer_output'\n",
    "infer_train_result_dir = 'train_y'\n",
    "infer_test_result_dir = 'test_y'\n",
    "\n",
    "def load_y_data(prefix=infer_result_dir_prefix, y_data_dir=infer_train_result_dir):\n",
    "    cwd = os.getcwd()\n",
    "    flist = glob.glob(os.path.join(os.path.join(os.path.join(cwd, prefix), y_data_dir), '*.p.npy'))\n",
    "    y_data = {}\n",
    "    for f in flist:\n",
    "        fid = os.path.basename(f).split('.')[0]\n",
    "        assert fid not in y_data\n",
    "        y_data[fid] = np.load(f)\n",
    "    return y_data\n",
    "\n",
    "infer_y_train = load_y_data(prefix=infer_result_dir_prefix, y_data_dir=infer_train_result_dir)\n",
    "print (\"Training y data size: {}\".format(len(infer_y_train)))\n",
    "\n",
    "infer_y_test = load_y_data(prefix=infer_result_dir_prefix, y_data_dir=infer_test_result_dir)\n",
    "print (\"Test y data size: {}\".format(len(infer_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape on an example: (100, 6)\n",
      "Example 1 index: [  8.08266602e+02   5.53883743e+01   9.28702026e+02   3.41990082e+02\n",
      "   3.04418266e-01   3.90000000e+01]\n",
      "Image size: 96499\n",
      "Train and test len: 105559\n"
     ]
    }
   ],
   "source": [
    "k = list(infer_y_train.keys())[0]\n",
    "print (\"Shape on an example: {}\".format(infer_y_train[k].shape))\n",
    "print (\"Example 1 index: {}\".format(infer_y_train[k][0]))\n",
    "\n",
    "image_size_cache_file='image_size_cache.p'\n",
    "with open(image_size_cache_file, 'rb') as f:\n",
    "    image_size_cache = pickle.load(f)\n",
    "    \n",
    "# All images covered.\n",
    "print (\"Image size: {}\".format(len(image_size_cache)))\n",
    "print (\"Train and test len: {}\".format(len(infer_y_train) + len(infer_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the training and test set for our model using\n",
    "\n",
    "Using the inference output and iterating over the inference output of training and test data\n",
    "create the training and test set for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the pickle files corresponding to the orignal train test set split.\n",
    "fullsize_train_model_file = 'fullsize_train_model.p'\n",
    "fullsize_test_model_file = 'fullsize_test_model.p'\n",
    "\n",
    "with open(fullsize_train_model_file, 'rb') as f:\n",
    "    fullsize_train_model_data = pickle.load(f)\n",
    "with open(fullsize_test_model_file, 'rb') as f:\n",
    "    fullsize_test_model_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of loaded data: dict_keys(['boxes', 'features', 'labels_orig'])\n",
      "Example 1: features: /home/powell/work/scpd/vision_project/visual_relationship/data/processed/raw/train_07/241d4c0143f2c91f.jpg\n",
      "Example 1: boxes: ['0', '0.88625', '0.4283019', '0.9981132', '0', '0.88625', '0.4283019', '0.9981132']\n",
      "Example 1: labels_orig: ('/m/04bcr3', '/m/083vt', 'is')\n",
      "Size of training set: 77325\n",
      "Size of test set: 16645\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the training dict and create training set\n",
    "print ('Keys of loaded data: {}'.format(fullsize_train_model_data.keys()))\n",
    "print ('Example 1: features: {}'.format(fullsize_train_model_data['features'][0]))\n",
    "print ('Example 1: boxes: {}'.format(fullsize_train_model_data['boxes'][0]))\n",
    "print ('Example 1: labels_orig: {}'.format(fullsize_train_model_data['labels_orig'][0]))\n",
    "\n",
    "def group_by_image(model_data):\n",
    "    image_group = {}\n",
    "    for image_path, boxes, labels in zip(model_data['features'], \n",
    "                                         model_data['boxes'],\n",
    "                                         model_data['labels_orig']):\n",
    "        fid = os.path.basename(image_path).split('.')[0]\n",
    "        if fid not in image_group:\n",
    "            image_group[fid] = {}\n",
    "\n",
    "        triplet = (labels[0], labels[1], labels[2])\n",
    "        if triplet not in image_group[fid]:\n",
    "            image_group[fid][triplet] = []\n",
    "            \n",
    "        image_group[fid][triplet].append(boxes)\n",
    "    image_group_triplet_sorted = {}\n",
    "    for k, v in image_group.items():\n",
    "        l = sorted(v.items(), key=lambda kv: len(kv[1]), reverse=True)\n",
    "        image_group_triplet_sorted[k] = [l[0]]  # Take the best triplet only :(\n",
    "    return image_group_triplet_sorted         \n",
    "\n",
    "        \n",
    "def create_dataset_from_infer(grouped_model_data, y_data, shape_restrict=(100, 6)):\n",
    "    dataset = {'id': [], 'features': [], 'boxes': [], 'labels_orig': [], \n",
    "               'label_subject_orig': [], 'label_object_orig': [], 'label_rel_orig': []}\n",
    "    for fid, triplet_list in grouped_model_data.items():\n",
    "        #assert fid not in y_data\n",
    "        labels = triplet_list[0][0]  # Most used triplet labels only\n",
    "        boxes = triplet_list[0][1]   # Most used triplet boxes only\n",
    "        if fid in y_data and y_data[fid].shape == shape_restrict:\n",
    "            dataset['id'].append(fid)\n",
    "            dataset['features'].append(y_data[fid].reshape(-1))\n",
    "            dataset['boxes'].append(np.asarray(boxes))\n",
    "            dataset['labels_orig'].append(labels)\n",
    "            dataset['label_subject_orig'].append(labels[0])\n",
    "            dataset['label_object_orig'].append(labels[1])\n",
    "            dataset['label_rel_orig'].append(labels[2])\n",
    "    return dataset\n",
    "\n",
    "model_train_grouped = group_by_image(model_data=fullsize_train_model_data)\n",
    "model_train_dataset = create_dataset_from_infer(grouped_model_data=model_train_grouped, y_data=infer_y_train)\n",
    "print ('Size of training set: {}'.format(len(model_train_dataset['id'])))\n",
    "\n",
    "model_test_grouped = group_by_image(model_data=fullsize_test_model_data)\n",
    "model_test_dataset = create_dataset_from_infer(grouped_model_data=model_test_grouped, y_data=infer_y_test)\n",
    "print ('Size of test set: {}'.format(len(model_test_dataset['id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training triplets length: 298\n",
      "Model test triplets length: 268\n"
     ]
    }
   ],
   "source": [
    "def get_triplets(model_grouped):\n",
    "    ret_dict = collections.defaultdict(int)\n",
    "    for k, v in model_grouped.items():\n",
    "        ret_dict[v[0][0]] += 1\n",
    "    return ret_dict\n",
    "\n",
    "model_train_triplet_dict = get_triplets(model_train_grouped)\n",
    "model_test_triplet_dict = get_triplets(model_test_grouped)\n",
    "\n",
    "print ('Model training triplets length: {}'.format(len(model_train_triplet_dict.keys())))\n",
    "print ('Model test triplets length: {}'.format(len(model_test_triplet_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training triplets length: 292\n",
      "Model test triplets length: 231\n",
      "Model training relationships: [('is', 39807), ('at', 16890), ('holds', 7917), ('on', 6814), ('interacts_with', 2062), ('plays', 2014), ('inside_of', 1232), ('hits', 409), ('wears', 169), ('under', 11)]\n",
      "Model test relationships: [('is', 8090), ('at', 4799), ('on', 1454), ('holds', 1243), ('plays', 481), ('interacts_with', 285), ('inside_of', 199), ('hits', 57), ('wears', 33), ('under', 4)]\n"
     ]
    }
   ],
   "source": [
    "def get_triplets_from_dataset(model_dataset):\n",
    "    ret_dict = collections.defaultdict(int)\n",
    "    rel_dict = collections.defaultdict(int)\n",
    "    for t in model_dataset['labels_orig']:\n",
    "        ret_dict[t] += 1\n",
    "        rel_dict[t[2]] += 1\n",
    "    return ret_dict, rel_dict\n",
    "\n",
    "model_train_triplet_dict, model_train_rel_dict = get_triplets_from_dataset(model_train_dataset)\n",
    "model_test_triplet_dict, model_test_rel_dict = get_triplets_from_dataset(model_test_dataset)\n",
    "\n",
    "print ('Model training triplets length: {}'.format(len(model_train_triplet_dict.keys())))\n",
    "print ('Model test triplets length: {}'.format(len(model_test_triplet_dict.keys())))\n",
    "\n",
    "print ('Model training relationships: {}'.format(sorted(model_train_rel_dict.items(), key=lambda kv: kv[1], reverse=True)))\n",
    "print ('Model test relationships: {}'.format(sorted(model_test_rel_dict.items(), key=lambda kv: kv[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set size: 77325\n",
      "Test data set size: 16645\n",
      "Before Multi-Encoding feature shape: (600,), label: ('/m/05r655', '/m/0cvnqh', 'on'), label_subject: /m/05r655,        label_object: /m/0cvnqh, label_relationship: on\n",
      "After Multi-Encoding feature shape: (600,), subject label: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "MLB classes size object/subject: 61, classes: ['/m/01226z' '/m/01599' '/m/01940j' '/m/0199g' '/m/019w40' '/m/01_5g'\n",
      " '/m/01bl7v' '/m/01f91_' '/m/01mzpv' '/m/01s55n' '/m/01y9k5' '/m/01yrx'\n",
      " '/m/026t6' '/m/029bxz' '/m/02hj4' '/m/02jvh9' '/m/02p5f1q' '/m/0342h'\n",
      " '/m/03bt1vf' '/m/03k3r' '/m/03m3pdh' '/m/03qrc' '/m/03ssj5' '/m/04_sv'\n",
      " '/m/04bcr3' '/m/04ctx' '/m/04dr76w' '/m/04lbp' '/m/04yx4' '/m/050k8'\n",
      " '/m/0584n8' '/m/05_5p_0' '/m/05ctyq' '/m/05r5c' '/m/05r655' '/m/05z87'\n",
      " '/m/06__v' '/m/071p9' '/m/078jl' '/m/078n6m' '/m/07y_7' '/m/080hkjn'\n",
      " '/m/083vt' '/m/08pbxl' '/m/09tvcd' '/m/0bt9lr' '/m/0bwd_0j' '/m/0cmx8'\n",
      " '/m/0cvnqh' '/m/0dnr7' '/m/0dt3t' '/m/0dv5r' '/m/0dv9c' '/m/0fx9l'\n",
      " '/m/0h2r6' '/m/0h8my_4' '/m/0hg7b' '/m/0k4j' '/m/0l14j_' '/m/0pg52'\n",
      " '/m/0wdt60w']\n",
      "MLB classes size relationship: 10, classes: ['at' 'hits' 'holds' 'inside_of' 'interacts_with' 'is' 'on' 'plays' 'under'\n",
      " 'wears']\n",
      "Training data label set size: 77325\n",
      "Test data label set size: 16645\n"
     ]
    }
   ],
   "source": [
    "print ('Training data set size: {}'.format(len(model_train_dataset['features'])))\n",
    "print ('Test data set size: {}'.format(len(model_test_dataset['features'])))  \n",
    "\n",
    "classes_file='fullsize_classes.csv'\n",
    "label_tag_to_name = process_labels_from_csv_input()\n",
    "labels_to_names = load_labels(label_tag_to_name, labels_file=classes_file)\n",
    "\n",
    "print ('Before Multi-Encoding feature shape: {}, label: {}, label_subject: {}, \\\n",
    "       label_object: {}, label_relationship: {}'.format(model_train_dataset['features'][0].shape, \n",
    "                                                        model_test_dataset['labels_orig'][0],\n",
    "                                                        model_test_dataset['label_subject_orig'][0],\n",
    "                                                        model_test_dataset['label_object_orig'][0],\n",
    "                                                        model_test_dataset['label_rel_orig'][0]))\n",
    "\n",
    "# label encoding\n",
    "mlb_subject_object = LabelBinarizer()\n",
    "mlb_subject_object = mlb_subject_object.fit([ v[1] for k, v in labels_to_names.items() ])\n",
    "\n",
    "mlb_relationship = LabelBinarizer()\n",
    "mlb_relationship = mlb_relationship.fit(model_train_dataset['label_rel_orig'] + \n",
    "                                        model_test_dataset['label_rel_orig'])\n",
    "         \n",
    "# 1 hot vector encoding for subject/object\n",
    "model_train_dataset['label_subject'] = mlb_subject_object.transform(model_train_dataset['label_subject_orig'])\n",
    "model_test_dataset['label_subject'] = mlb_subject_object.transform(model_test_dataset['label_subject_orig'])\n",
    "model_train_dataset['label_object'] = mlb_subject_object.transform(model_train_dataset['label_subject_orig'])\n",
    "model_test_dataset['label_object'] = mlb_subject_object.transform(model_test_dataset['label_subject_orig'])\n",
    "\n",
    "# 1 hot vector encoding for relationship\n",
    "model_train_dataset['label_rel'] = mlb_relationship.transform(model_train_dataset['label_rel_orig'])\n",
    "model_test_dataset['label_rel'] = mlb_relationship.transform(model_test_dataset['label_rel_orig'])\n",
    "                  \n",
    "print ('After Multi-Encoding feature shape: {}, subject label: {}'.format(model_train_dataset['features'][0].shape, \n",
    "                                                                 model_train_dataset['label_subject'][0]))\n",
    "print('MLB classes size object/subject: {}, classes: {}'.format(len(mlb_subject_object.classes_), \n",
    "                                                                mlb_subject_object.classes_))\n",
    "print('MLB classes size relationship: {}, classes: {}'.format(len(mlb_relationship.classes_), \n",
    "                                                                mlb_relationship.classes_))\n",
    "print ('Training data label set size: {}'.format(len(model_train_dataset['label_subject'])))\n",
    "print ('Test data label set size: {}'.format(len(model_test_dataset['label_subject']))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling - Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (77325, 600)\n",
      "Boxes shape: (1, 8)\n",
      "Subject labels shape: (61,)\n",
      "Object labels shape: (61,)\n",
      "Relationship labels shape: (10,)\n",
      "Shape of y_test[0]: (77325,), y_test[1]: (77325,)\n",
      "Shape of X_test: (16645, 600)\n",
      "Shape of y_test[0]: (77325,), y_test[1]: (16645, 61), y_test[2]: (16645, 61), y_test[3]: (16645, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.asarray(model_train_dataset['features'])\n",
    "print('Shape of X_train: {}'.format(X_train.shape))\n",
    "print ('Boxes shape: {}'.format(model_train_dataset['boxes'][0].shape))\n",
    "print ('Subject labels shape: {}'.format(model_train_dataset['label_subject'][0].shape))\n",
    "print ('Object labels shape: {}'.format(model_train_dataset['label_object'][0].shape))\n",
    "print ('Relationship labels shape: {}'.format(model_train_dataset['label_rel'][0].shape))\n",
    "\n",
    "# Sequence of labels are boxes, subject, object and relationship\n",
    "y_train = [np.asarray(model_train_dataset['boxes']), \n",
    "           np.asarray(model_train_dataset['label_subject']),\n",
    "           np.asarray(model_train_dataset['label_object']),\n",
    "           np.asarray(model_train_dataset['label_rel'])]\n",
    "\n",
    "print('Shape of y_test[0]: {}, y_test[1]: {}'.format(y_train[0].shape, y_train[0].shape))\n",
    "X_test = np.asarray(model_test_dataset['features'])\n",
    "print('Shape of X_test: {}'.format(X_test.shape))\n",
    "y_test = [np.asarray(model_train_dataset['boxes']), \n",
    "          np.array(model_test_dataset['label_subject']),\n",
    "          np.array(model_test_dataset['label_object']),\n",
    "          np.array(model_test_dataset['label_rel'])]\n",
    "print('Shape of y_test[0]: {}, y_test[1]: {}, y_test[2]: {}, y_test[3]: {}'.format(\n",
    "    y_test[0].shape, y_test[1].shape, y_test[2].shape, y_test[3].shape))\n",
    "\n",
    "std_scale = StandardScaler().fit(X_train)\n",
    "X_train = std_scale.transform(X_train)\n",
    "X_test = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train 1: [ 2.30476513 -1.55111804  0.946195   -2.21375896 -1.63143538  1.11326003\n",
      "  1.62579324 -0.73336721  1.14203743 -2.08839704 -1.60753007  0.03989343\n",
      "  1.46834614 -0.98847048  0.61472989 -2.07187138 -1.44191008 -1.537494\n",
      "  1.93715409 -1.4439172   1.09913769 -1.86660273 -1.12902393  0.21264154\n",
      "  1.60286758 -1.08196701  1.19581253 -1.87437201 -1.50277761 -1.47240353\n",
      "  1.89759088 -1.54828243  1.10862103 -1.81480853 -1.38197016 -0.39240401\n",
      " -1.03869305  0.37518626 -1.99367051 -0.87552212 -1.34316586 -1.43730992\n",
      "  1.92030143 -1.32616157  1.11024295 -1.82241732 -1.35340623 -1.42473504\n",
      "  1.40007982 -0.95560896  1.25338824 -1.84158927 -1.20666618 -1.16058621\n",
      "  1.8548597  -1.55079528  1.12964541 -1.77002157 -1.18894275  1.01615254\n",
      " -0.38454295 -0.14345482  1.39025287  0.17025324 -1.08411491 -1.13931352\n",
      "  1.43674169 -1.3108527   1.16041195 -1.77430797 -1.10298545  0.28150981\n",
      "  1.97443884 -0.79628096  1.13408748 -1.73078695 -0.9932918   0.03935141\n",
      "  0.98225983 -0.30963223  1.50655739  0.25793288 -1.13993928  0.16555823\n",
      "  1.88240417 -0.98222149  1.18316481 -1.93844092 -1.09971084  0.72951415\n",
      "  1.38041665 -1.09853334  0.65816643 -2.17497866 -1.06277085 -1.36148474\n",
      "  0.63412118 -1.43244353  0.25268193 -1.62767758 -1.06188177 -1.36007208\n",
      "  1.48156315 -1.17738818  1.24845112 -1.66485715 -1.00829936  0.97762692\n",
      "  1.89124829 -1.05342743  1.32481133 -1.77018459 -1.07322272 -1.35362651\n",
      "  1.31532328 -1.11494354  0.92594028 -1.73028018 -1.20534591  0.18764374\n",
      "  1.48516943 -1.57548006  1.06126348 -1.8935266  -1.16856159  0.98985935\n",
      "  1.08486564  0.59891138  0.82717322  0.02142671 -1.17571562 -1.34362003\n",
      "  1.91267313 -0.75478596  1.15938629 -1.69512695 -1.16572543  0.74570539\n",
      "  1.63542592 -1.75274483  1.40952983 -1.34502769 -1.14511768  0.99028316\n",
      "  1.74566322 -1.02793974  1.41034744 -1.57281824 -1.09158221  0.19309839\n",
      "  1.70683352 -1.54981536  1.37910716 -1.46318365 -1.05552513 -1.34170107\n",
      "  1.73743861 -1.585911    1.20260083 -2.35579041 -1.13577889  0.9946534\n",
      " -0.20428925 -0.1990985   1.27467126  0.07554122 -1.12649342  0.20101845\n",
      "  1.25536406 -1.49788018  1.07224392 -1.73622563 -1.10607647 -1.32891584\n",
      "  2.20082765 -1.14535696  1.55718894 -2.01170104 -1.07577834 -1.33074133\n",
      "  1.90026151 -1.452902    1.26155454 -1.70481452 -1.05365624  0.98712999\n",
      "  0.68973213 -1.42758592  0.21560715 -2.54174847 -1.03624141 -1.32931595\n",
      "  1.29874776 -1.25737712  0.83332732 -1.67327986 -1.05298886  0.99542113\n",
      "  1.34298926 -1.07527195  0.86201078 -1.9014502  -1.01638367 -1.32264528\n",
      "  0.63811735 -1.25825915  1.52862152  0.0305973  -0.97779662 -1.08289889\n",
      "  0.61226475 -1.57698135  0.24716961 -1.46847717 -0.95150813 -0.89957117\n",
      " -0.90545912 -0.91281416  1.29232633 -0.17141268 -0.91822989 -1.0818971\n",
      "  1.69550575 -1.63080017  1.21171765 -2.30060809 -0.90539306  0.3245771\n",
      "  1.24361452 -1.120174    1.47048682 -0.8955431  -0.98899715  0.20461422\n",
      " -1.2252861   0.23416362 -1.2250622  -0.5887379  -0.96175128 -1.08014337\n",
      "  2.04667164 -1.09761186  1.5216942  -1.66392632 -0.93231222 -1.32249309\n",
      "  1.44740346 -0.90745886  0.90619096 -1.81738985 -0.90141251 -1.32364634\n",
      "  1.4004546  -1.218653    1.2924407  -1.51976256 -0.91195923 -0.27989644\n",
      "  1.23514052 -0.71181779  0.86339096 -1.7055822  -0.94652753  0.20301372\n",
      "  1.23477756  0.64057152  0.86213667 -0.15670412 -0.93229172 -1.3064726\n",
      "  1.5047837  -1.35701718  0.96420524 -1.69036239 -0.91805608 -1.30974761\n",
      "  1.24709129 -1.01916815  0.59682195 -1.71134401 -0.93523959 -1.31318761\n",
      " -1.30585977  0.2155493  -1.74309652 -0.54051537 -0.91621389 -1.07410151\n",
      "  1.95492086 -1.13851221  1.57570593 -1.56110276 -0.92125826  0.20931536\n",
      "  1.69659005 -1.39758876  1.11255151 -1.84292775 -0.97959092  1.0033315\n",
      "  1.54815992 -1.74451888  1.4496706  -1.13029573 -1.00073427  0.33444459\n",
      "  1.93920251 -1.43263807  1.56593405 -1.41680663 -0.98110821  0.32653138\n",
      "  1.90105375 -1.58567169  1.52645368 -1.44896789 -0.96790479  1.00661535\n",
      "  1.82940141 -0.91263315  1.20869133 -1.49817259 -0.94735825 -0.64323941\n",
      "  1.79494019 -1.10272888  1.22143391 -2.08715881 -0.93109677  0.75916118\n",
      "  1.65888908 -1.10399484  1.25102224 -2.06458675 -0.9432809   0.56941674\n",
      "  1.24392499 -1.27462556  0.85473837 -1.57814553 -0.92298337  0.33278283\n",
      "  1.67065582 -1.07398278  1.16227172 -1.93295303 -0.93454717  0.75876526\n",
      "  1.66974623 -1.41045061  1.11634463 -1.8208799  -0.91501826  0.33906632\n",
      "  1.16143002 -0.67318364  0.81646247 -1.60077592 -0.91099203 -1.07551544\n",
      "  1.82364266 -0.91088783  1.21760584 -1.48118062 -0.89473761  0.32516701\n",
      "  1.67001889 -1.10216569  1.21811837 -1.82733066 -0.87507057 -1.31161965\n",
      " -1.28365984  0.28286773 -1.73260469 -0.58116112 -0.8678786  -1.31084085\n",
      "  1.45893966 -1.66711627  1.0735079  -1.64394954 -0.87479267  0.33098977\n",
      "  0.79812007  0.22047261  1.33180716  0.40839802 -0.88805943 -1.06863812\n",
      "  1.19930937 -1.04481152  1.04950467 -1.49510726 -0.87149757 -1.37158822\n",
      "  1.20079392 -1.15864631  0.97651282 -1.54082099 -0.88826662  2.10131233\n",
      "  1.55512338 -1.31685618  1.17798715 -1.45366383 -0.88596672  0.21624019\n",
      "  1.29230765 -1.38399526  1.24225708 -1.48558    -0.89017935  1.05817639\n",
      "  0.14608217 -0.9835009   1.58119404 -0.06315952 -0.87191476  0.20517348\n",
      "  1.19447837 -0.65751286  1.02969594 -1.44977297 -0.8677965   0.21263816\n",
      "  1.68137368 -0.87773092  1.13604763 -1.72682745 -0.8549704   0.75617669\n",
      "  0.91066494 -0.82274113  1.3964451  -1.17386158 -0.83649118  0.20803136\n",
      "  1.53387119 -1.23113842  1.12903121 -1.90498432 -0.82279816 -1.3096131\n",
      "  0.84102439 -1.18592928  0.95752803 -1.48289135 -0.81564663 -1.30631967\n",
      "  1.26083144 -0.83024875  0.879941   -1.58840725 -0.79809897 -1.30301378\n",
      "  1.84045359 -0.99257151  1.19560917 -1.77429573 -0.79606354  0.75764019\n",
      "  1.49350872 -1.0237083   0.86419136 -1.77785317 -0.79601412 -1.304913\n",
      "  1.62243512 -0.98027118  1.20852162 -1.63938812 -0.77904193  0.09030466\n",
      "  1.19523604 -1.14887868  0.77012577 -1.79308129 -0.79713484  0.20490818\n",
      "  1.1606271  -0.6721457   0.68505065 -1.56164088 -0.80311966 -1.31555347\n",
      "  1.18977863 -1.40919696  0.99267516 -1.43881775 -0.83437658 -0.2776272\n",
      "  1.7671876  -1.32500915  1.23573141 -1.74586905 -0.82063727  0.33183049\n",
      "  1.81205235 -1.48407144  1.31138407 -1.53787491 -0.80716504  1.06693543\n",
      "  1.63676375 -1.41549678  1.22409664 -1.99855928 -0.81236271  1.36081516\n",
      "  1.82381603 -1.28893494  1.32517484 -1.44644831 -0.8016258   0.32835049\n",
      "  1.63845878 -1.41180042  1.23584687 -1.99804507 -0.81569766 -1.31385497\n",
      "  1.77970953 -0.92750689  1.22029602 -1.41646713 -0.81375996  1.36727771\n",
      "  1.61912951 -1.11377772  1.26905152 -1.97355001 -0.81724677  0.09501832\n",
      " -1.31180038  0.13693558 -1.0383203  -0.43198771 -0.82053198  0.21278332\n",
      "  1.22776278 -1.04523769  1.26772576 -1.509504   -0.8206297   2.27245455\n",
      "  1.48658618 -1.4103274   1.38277279 -1.90991542 -0.80889418  1.0054426\n",
      "  1.76257671 -1.43431807  1.34152631 -1.38602445 -0.80698368 -0.27487041\n",
      "  1.65910061 -1.64777433  1.22303513 -2.06616929 -0.80306723 -0.27816125\n",
      "  1.64630945 -1.67175561  1.24008425 -1.38032086 -0.79551822  1.36093878\n",
      " -0.02390616  0.45842862  1.57620377  0.87541906 -0.79246578 -1.05576022\n",
      "  1.61628342 -1.17971178  1.22524915 -1.85905948 -0.78237248 -0.88072319\n",
      "  1.14846771 -1.01942816  0.66897453 -1.49797115 -0.7700485  -1.05836291\n",
      "  1.58543156 -0.98301818  1.20545734 -1.59535048 -0.75986092 -0.62917313\n",
      "  1.86012257 -1.59640257  1.57194858 -1.34092392 -0.79277738 -1.30389849]\n",
      "Example test boxes 1: [['0.78125' '0.91875' '0.075833336' '0.4525' '0.675625' '0.955'\n",
      "  '0.16916667' '0.4425']]\n",
      "Example test subject labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example test object labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example test relationship labels: [0 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print ('Example train 1: {}'.format(X_train[0]))\n",
    "print ('Example test boxes 1: {}'.format(y_train[0][0]))\n",
    "print ('Example test subject labels: {}'.format(y_train[1][0]))\n",
    "print ('Example test object labels: {}'.format(y_train[2][0]))\n",
    "print ('Example test relationship labels: {}'.format(y_train[3][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PriorProbability(keras.initializers.Initializer):\n",
    "    \"\"\" Apply a prior probability to the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, probability=0.01):\n",
    "        self.probability = probability\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'probability': self.probability\n",
    "        }\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        # set bias to -log((1 - p)/p) for foreground\n",
    "        result = np.ones(shape, dtype=dtype) * -math.log((1 - self.probability) / self.probability)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DeepClassifyRegressModel(input_shape, subject_output_class_size, object_output_class_size,\n",
    "                             relationship_output_class_size,\n",
    "                             output_reg_size, prior_probability = 0.01, \n",
    "                             drop_out=0.5,\n",
    "                             disable_regress=True):\n",
    "    \"\"\"\n",
    "    Model is regular deep neural network with 4 outputs.\n",
    "    output1: Regression with two bounding boxes of size (N, 4)\n",
    "    output2: Object classifcation of size(N, 61)\n",
    "    output3: Subject classifcation of size(N, 61)\n",
    "    output3: Relationship classifcation of size(N, 10)\n",
    "    \"\"\"\n",
    "    main_input = Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1 Relu - Norm - Dropout\n",
    "    if disable_regress is False:\n",
    "        bias_initializer=PriorProbability(probability=prior_probability)\n",
    "    else:\n",
    "        bias_initializer='zeros'\n",
    "    shared_model = Dense(units=1024, input_shape=input_shape, \n",
    "                        kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                        bias_initializer=bias_initializer)(main_input)                   \n",
    "    shared_model = BatchNormalization()(shared_model)\n",
    "    shared_model = Activation('relu')(shared_model)\n",
    "    shared_model = Dropout(drop_out)(shared_model)\n",
    "    \n",
    "    # Layer 2\n",
    "    if disable_regress is False:\n",
    "        bias_initializer=PriorProbability(probability=prior_probability)\n",
    "    shared_model = Dense(units=256,\n",
    "                         kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                         bias_initializer=PriorProbability(probability=prior_probability))(shared_model)\n",
    "    shared_model = BatchNormalization()(shared_model)\n",
    "    shared_model = Activation('relu')(shared_model)\n",
    "    shared_model = Dropout(drop_out)(shared_model)\n",
    "    \n",
    "    # Layer 3\n",
    "    \"\"\"\n",
    "    if disable_regress is False:\n",
    "        bias_initializer=PriorProbability(probability=prior_probability)\n",
    "    shared_model = Dense(units=128,\n",
    "                         kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                         bias_initializer=PriorProbability(probability=prior_probability))(shared_model)\n",
    "    shared_model = BatchNormalization()(shared_model)\n",
    "    shared_model = Activation('relu')(shared_model)\n",
    "    shared_model = Dropout(drop_out)(shared_model)\n",
    "    \"\"\"\n",
    "\n",
    "    # Regression branch\n",
    "    model_regression = None\n",
    "    if disable_regress is False:\n",
    "        model_regression = Dense(output_reg_size, activation=\"linear\", name=\"regression\")(shared_model)\n",
    "    \n",
    "    # Subject classification branch\n",
    "    model_subject_classify = Dense(subject_output_class_size)(shared_model)\n",
    "    model_subject_classify = Activation(tf.nn.softmax, name=\"classification_subject\")(model_subject_classify)\n",
    "    \n",
    "    # Object classification branch\n",
    "    model_object_classify = Dense(object_output_class_size)(shared_model)\n",
    "    model_object_classify = Activation(tf.nn.softmax, name=\"classification_object\")(model_object_classify)\n",
    "    \n",
    "    # 2 Layer for relationship, everything is the input\n",
    "    if disable_regress is False:\n",
    "        concat_layer = concatenate([model_regression, model_subject_classify, model_object_classify], axis=1)\n",
    "    else:\n",
    "        concat_layer = concatenate([shared_model, model_subject_classify, model_object_classify], axis=1)\n",
    "\n",
    "    if disable_regress is False:\n",
    "        bias_initializer=PriorProbability(probability=prior_probability)\n",
    "    relationship_model = Dense(units=128,\n",
    "                         kernel_initializer=keras.initializers.normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                         bias_initializer=PriorProbability(probability=prior_probability))(concat_layer)\n",
    "    relationship_model = BatchNormalization()(relationship_model)\n",
    "    relationship_model = Activation('relu')(relationship_model)\n",
    "    relationship_model = Dropout(drop_out)(relationship_model)\n",
    "    \n",
    "    model_relationship_classify = Dense(relationship_output_class_size)(relationship_model)\n",
    "    model_relationship_classify = Activation(tf.nn.softmax, \n",
    "                                             name=\"classification_relationship\")(model_relationship_classify)\n",
    "    \n",
    "    outputs=[]\n",
    "    if model_regression is not None:\n",
    "        outputs=[model_regression]\n",
    "    model = Model(inputs=[main_input], outputs= outputs + [model_subject_classify, \n",
    "                                                model_object_classify, model_relationship_classify])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compatible with tensorflow backend\n",
    "\n",
    "our_model.compile(optimizer=optimizer, loss=[focal_loss(alpha=.25, gamma=2)])\n",
    "'''\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet import losses\n",
    "def train_model(model, x_train, y_train, learn_rate=1e-5, epochs=32, batch_size=64, \n",
    "                disable_regress = True, verbose=1):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    Using as loss function and Adam optimizer\n",
    "    \"\"\"\n",
    "    loss_dict = {'classification_subject': focal_loss(), # 'categorical_crossentropy', \n",
    "            'classification_object': focal_loss(),  #'categorical_crossentropy',  \n",
    "            'classification_relationship': focal_loss(), # 'categorical_crossentropy' \n",
    "                }\n",
    "    if disable_regress is False:\n",
    "       loss_dict['regression'] = 'mean_squared_error'\n",
    "    \n",
    "    model.compile(loss=loss_dict,\n",
    "        optimizer=keras.optimizers.adam(lr=learn_rate, clipnorm=0.001), metrics=['accuracy'])\n",
    "    return model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "        verbose=verbose, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (600,)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1024)         615424      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1024)         4096        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 1024)         0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1024)         0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 256)          262400      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256)          1024        dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 256)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 256)          0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 61)           15677       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 61)           15677       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "classification_subject (Activat (None, 61)           0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classification_object (Activati (None, 61)           0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 378)          0           dropout_14[0][0]                 \n",
      "                                                                 classification_subject[0][0]     \n",
      "                                                                 classification_object[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          48512       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128)          512         dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 128)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 128)          0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 10)           1290        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "classification_relationship (Ac (None, 10)           0           dense_30[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 964,612\n",
      "Trainable params: 961,796\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n",
      "Train on 61860 samples, validate on 15465 samples\n",
      "Epoch 1/50\n",
      "61860/61860 [==============================] - 27s 440us/step - loss: 78.5693 - classification_subject_loss: 31.4543 - classification_object_loss: 28.9081 - classification_relationship_loss: 18.2069 - classification_subject_acc: 0.1125 - classification_object_acc: 0.1456 - classification_relationship_acc: 0.2327 - val_loss: 55.0298 - val_classification_subject_loss: 22.4813 - val_classification_object_loss: 21.1256 - val_classification_relationship_loss: 11.4229 - val_classification_subject_acc: 0.3250 - val_classification_object_acc: 0.3446 - val_classification_relationship_acc: 0.4425\n",
      "Epoch 2/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 58.3005 - classification_subject_loss: 22.3093 - classification_object_loss: 21.5172 - classification_relationship_loss: 14.4740 - classification_subject_acc: 0.2707 - classification_object_acc: 0.2733 - classification_relationship_acc: 0.3415 - val_loss: 44.7732 - val_classification_subject_loss: 17.8196 - val_classification_object_loss: 17.2374 - val_classification_relationship_loss: 9.7163 - val_classification_subject_acc: 0.3684 - val_classification_object_acc: 0.3691 - val_classification_relationship_acc: 0.4973\n",
      "Epoch 3/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 51.1003 - classification_subject_loss: 19.3327 - classification_object_loss: 18.9030 - classification_relationship_loss: 12.8646 - classification_subject_acc: 0.3025 - classification_object_acc: 0.3005 - classification_relationship_acc: 0.3838 - val_loss: 39.6684 - val_classification_subject_loss: 15.5451 - val_classification_object_loss: 15.2407 - val_classification_relationship_loss: 8.8826 - val_classification_subject_acc: 0.3895 - val_classification_object_acc: 0.3884 - val_classification_relationship_acc: 0.5185\n",
      "Epoch 4/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 47.1630 - classification_subject_loss: 17.7336 - classification_object_loss: 17.5535 - classification_relationship_loss: 11.8759 - classification_subject_acc: 0.3150 - classification_object_acc: 0.3098 - classification_relationship_acc: 0.4008 - val_loss: 36.7448 - val_classification_subject_loss: 14.2503 - val_classification_object_loss: 14.1065 - val_classification_relationship_loss: 8.3879 - val_classification_subject_acc: 0.3947 - val_classification_object_acc: 0.3958 - val_classification_relationship_acc: 0.5273\n",
      "Epoch 5/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 44.4640 - classification_subject_loss: 16.7963 - classification_object_loss: 16.6564 - classification_relationship_loss: 11.0113 - classification_subject_acc: 0.3175 - classification_object_acc: 0.3152 - classification_relationship_acc: 0.4150 - val_loss: 34.8867 - val_classification_subject_loss: 13.5212 - val_classification_object_loss: 13.4262 - val_classification_relationship_loss: 7.9392 - val_classification_subject_acc: 0.4022 - val_classification_object_acc: 0.4023 - val_classification_relationship_acc: 0.5306\n",
      "Epoch 6/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 42.4929 - classification_subject_loss: 16.0885 - classification_object_loss: 15.9748 - classification_relationship_loss: 10.4297 - classification_subject_acc: 0.3210 - classification_object_acc: 0.3190 - classification_relationship_acc: 0.4253 - val_loss: 33.8209 - val_classification_subject_loss: 13.1072 - val_classification_object_loss: 13.0471 - val_classification_relationship_loss: 7.6666 - val_classification_subject_acc: 0.4059 - val_classification_object_acc: 0.4038 - val_classification_relationship_acc: 0.5357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 40.9839 - classification_subject_loss: 15.5222 - classification_object_loss: 15.4720 - classification_relationship_loss: 9.9896 - classification_subject_acc: 0.3258 - classification_object_acc: 0.3237 - classification_relationship_acc: 0.4288 - val_loss: 32.9826 - val_classification_subject_loss: 12.7399 - val_classification_object_loss: 12.7455 - val_classification_relationship_loss: 7.4973 - val_classification_subject_acc: 0.4109 - val_classification_object_acc: 0.4084 - val_classification_relationship_acc: 0.5423\n",
      "Epoch 8/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 39.6889 - classification_subject_loss: 15.0720 - classification_object_loss: 15.0176 - classification_relationship_loss: 9.5994 - classification_subject_acc: 0.3314 - classification_object_acc: 0.3300 - classification_relationship_acc: 0.4347 - val_loss: 32.2389 - val_classification_subject_loss: 12.4510 - val_classification_object_loss: 12.4347 - val_classification_relationship_loss: 7.3532 - val_classification_subject_acc: 0.4119 - val_classification_object_acc: 0.4111 - val_classification_relationship_acc: 0.5465\n",
      "Epoch 9/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 38.6017 - classification_subject_loss: 14.7224 - classification_object_loss: 14.6151 - classification_relationship_loss: 9.2642 - classification_subject_acc: 0.3382 - classification_object_acc: 0.3360 - classification_relationship_acc: 0.4450 - val_loss: 31.7133 - val_classification_subject_loss: 12.2133 - val_classification_object_loss: 12.2275 - val_classification_relationship_loss: 7.2724 - val_classification_subject_acc: 0.4149 - val_classification_object_acc: 0.4147 - val_classification_relationship_acc: 0.5501\n",
      "Epoch 10/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 37.6469 - classification_subject_loss: 14.3503 - classification_object_loss: 14.3205 - classification_relationship_loss: 8.9761 - classification_subject_acc: 0.3449 - classification_object_acc: 0.3409 - classification_relationship_acc: 0.4546 - val_loss: 31.3698 - val_classification_subject_loss: 12.0911 - val_classification_object_loss: 12.0944 - val_classification_relationship_loss: 7.1844 - val_classification_subject_acc: 0.4180 - val_classification_object_acc: 0.4166 - val_classification_relationship_acc: 0.5529\n",
      "Epoch 11/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 37.0315 - classification_subject_loss: 14.1389 - classification_object_loss: 14.1366 - classification_relationship_loss: 8.7560 - classification_subject_acc: 0.3445 - classification_object_acc: 0.3428 - classification_relationship_acc: 0.4584 - val_loss: 31.1064 - val_classification_subject_loss: 12.0029 - val_classification_object_loss: 11.9776 - val_classification_relationship_loss: 7.1259 - val_classification_subject_acc: 0.4202 - val_classification_object_acc: 0.4187 - val_classification_relationship_acc: 0.5535\n",
      "Epoch 12/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 36.2656 - classification_subject_loss: 13.8719 - classification_object_loss: 13.8523 - classification_relationship_loss: 8.5414 - classification_subject_acc: 0.3498 - classification_object_acc: 0.3484 - classification_relationship_acc: 0.4635 - val_loss: 30.7725 - val_classification_subject_loss: 11.8532 - val_classification_object_loss: 11.8556 - val_classification_relationship_loss: 7.0637 - val_classification_subject_acc: 0.4216 - val_classification_object_acc: 0.4219 - val_classification_relationship_acc: 0.5565\n",
      "Epoch 13/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 35.6616 - classification_subject_loss: 13.6383 - classification_object_loss: 13.6583 - classification_relationship_loss: 8.3650 - classification_subject_acc: 0.3572 - classification_object_acc: 0.3548 - classification_relationship_acc: 0.4722 - val_loss: 30.5703 - val_classification_subject_loss: 11.7810 - val_classification_object_loss: 11.7813 - val_classification_relationship_loss: 7.0080 - val_classification_subject_acc: 0.4228 - val_classification_object_acc: 0.4230 - val_classification_relationship_acc: 0.5587\n",
      "Epoch 14/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 35.0293 - classification_subject_loss: 13.4186 - classification_object_loss: 13.4121 - classification_relationship_loss: 8.1986 - classification_subject_acc: 0.3614 - classification_object_acc: 0.3576 - classification_relationship_acc: 0.4806 - val_loss: 30.3280 - val_classification_subject_loss: 11.6862 - val_classification_object_loss: 11.6892 - val_classification_relationship_loss: 6.9526 - val_classification_subject_acc: 0.4248 - val_classification_object_acc: 0.4239 - val_classification_relationship_acc: 0.5593\n",
      "Epoch 15/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 34.4587 - classification_subject_loss: 13.2093 - classification_object_loss: 13.1948 - classification_relationship_loss: 8.0546 - classification_subject_acc: 0.3663 - classification_object_acc: 0.3650 - classification_relationship_acc: 0.4839 - val_loss: 30.1470 - val_classification_subject_loss: 11.6121 - val_classification_object_loss: 11.6188 - val_classification_relationship_loss: 6.9162 - val_classification_subject_acc: 0.4256 - val_classification_object_acc: 0.4243 - val_classification_relationship_acc: 0.5591\n",
      "Epoch 16/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 34.0776 - classification_subject_loss: 13.0525 - classification_object_loss: 13.0605 - classification_relationship_loss: 7.9646 - classification_subject_acc: 0.3680 - classification_object_acc: 0.3656 - classification_relationship_acc: 0.4906 - val_loss: 29.9721 - val_classification_subject_loss: 11.5530 - val_classification_object_loss: 11.5488 - val_classification_relationship_loss: 6.8703 - val_classification_subject_acc: 0.4286 - val_classification_object_acc: 0.4266 - val_classification_relationship_acc: 0.5635\n",
      "Epoch 17/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 33.6802 - classification_subject_loss: 12.9099 - classification_object_loss: 12.9035 - classification_relationship_loss: 7.8668 - classification_subject_acc: 0.3727 - classification_object_acc: 0.3725 - classification_relationship_acc: 0.4962 - val_loss: 29.7866 - val_classification_subject_loss: 11.4803 - val_classification_object_loss: 11.4813 - val_classification_relationship_loss: 6.8250 - val_classification_subject_acc: 0.4308 - val_classification_object_acc: 0.4291 - val_classification_relationship_acc: 0.5633\n",
      "Epoch 18/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 33.3233 - classification_subject_loss: 12.7599 - classification_object_loss: 12.7734 - classification_relationship_loss: 7.7901 - classification_subject_acc: 0.3767 - classification_object_acc: 0.3762 - classification_relationship_acc: 0.5003 - val_loss: 29.6163 - val_classification_subject_loss: 11.4084 - val_classification_object_loss: 11.4039 - val_classification_relationship_loss: 6.8040 - val_classification_subject_acc: 0.4293 - val_classification_object_acc: 0.4292 - val_classification_relationship_acc: 0.5661\n",
      "Epoch 19/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 32.8881 - classification_subject_loss: 12.6240 - classification_object_loss: 12.5817 - classification_relationship_loss: 7.6824 - classification_subject_acc: 0.3827 - classification_object_acc: 0.3821 - classification_relationship_acc: 0.5060 - val_loss: 29.5028 - val_classification_subject_loss: 11.3673 - val_classification_object_loss: 11.3784 - val_classification_relationship_loss: 6.7571 - val_classification_subject_acc: 0.4337 - val_classification_object_acc: 0.4310 - val_classification_relationship_acc: 0.5683\n",
      "Epoch 20/50\n",
      "61860/61860 [==============================] - 26s 416us/step - loss: 32.5878 - classification_subject_loss: 12.5103 - classification_object_loss: 12.4919 - classification_relationship_loss: 7.5856 - classification_subject_acc: 0.3834 - classification_object_acc: 0.3856 - classification_relationship_acc: 0.5105 - val_loss: 29.3612 - val_classification_subject_loss: 11.3287 - val_classification_object_loss: 11.3233 - val_classification_relationship_loss: 6.7093 - val_classification_subject_acc: 0.4334 - val_classification_object_acc: 0.4316 - val_classification_relationship_acc: 0.5695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 32.1979 - classification_subject_loss: 12.3230 - classification_object_loss: 12.3428 - classification_relationship_loss: 7.5321 - classification_subject_acc: 0.3899 - classification_object_acc: 0.3872 - classification_relationship_acc: 0.5170 - val_loss: 29.2167 - val_classification_subject_loss: 11.2720 - val_classification_object_loss: 11.2680 - val_classification_relationship_loss: 6.6767 - val_classification_subject_acc: 0.4340 - val_classification_object_acc: 0.4339 - val_classification_relationship_acc: 0.5734\n",
      "Epoch 22/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 31.8853 - classification_subject_loss: 12.2300 - classification_object_loss: 12.2159 - classification_relationship_loss: 7.4394 - classification_subject_acc: 0.3942 - classification_object_acc: 0.3946 - classification_relationship_acc: 0.5220 - val_loss: 29.1074 - val_classification_subject_loss: 11.2225 - val_classification_object_loss: 11.2288 - val_classification_relationship_loss: 6.6561 - val_classification_subject_acc: 0.4374 - val_classification_object_acc: 0.4345 - val_classification_relationship_acc: 0.5716\n",
      "Epoch 23/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 31.6149 - classification_subject_loss: 12.1224 - classification_object_loss: 12.1190 - classification_relationship_loss: 7.3735 - classification_subject_acc: 0.3982 - classification_object_acc: 0.3952 - classification_relationship_acc: 0.5250 - val_loss: 29.0172 - val_classification_subject_loss: 11.1950 - val_classification_object_loss: 11.2022 - val_classification_relationship_loss: 6.6199 - val_classification_subject_acc: 0.4369 - val_classification_object_acc: 0.4358 - val_classification_relationship_acc: 0.5764\n",
      "Epoch 24/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 31.4279 - classification_subject_loss: 12.0509 - classification_object_loss: 12.0556 - classification_relationship_loss: 7.3214 - classification_subject_acc: 0.3998 - classification_object_acc: 0.3983 - classification_relationship_acc: 0.5281 - val_loss: 28.9364 - val_classification_subject_loss: 11.1743 - val_classification_object_loss: 11.1690 - val_classification_relationship_loss: 6.5931 - val_classification_subject_acc: 0.4375 - val_classification_object_acc: 0.4356 - val_classification_relationship_acc: 0.5779\n",
      "Epoch 25/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 31.1458 - classification_subject_loss: 11.9305 - classification_object_loss: 11.9404 - classification_relationship_loss: 7.2749 - classification_subject_acc: 0.4038 - classification_object_acc: 0.4034 - classification_relationship_acc: 0.5324 - val_loss: 28.8071 - val_classification_subject_loss: 11.1218 - val_classification_object_loss: 11.1246 - val_classification_relationship_loss: 6.5607 - val_classification_subject_acc: 0.4380 - val_classification_object_acc: 0.4357 - val_classification_relationship_acc: 0.5793\n",
      "Epoch 26/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 30.8920 - classification_subject_loss: 11.8416 - classification_object_loss: 11.8578 - classification_relationship_loss: 7.1926 - classification_subject_acc: 0.4067 - classification_object_acc: 0.4063 - classification_relationship_acc: 0.5375 - val_loss: 28.7230 - val_classification_subject_loss: 11.0906 - val_classification_object_loss: 11.0996 - val_classification_relationship_loss: 6.5328 - val_classification_subject_acc: 0.4395 - val_classification_object_acc: 0.4368 - val_classification_relationship_acc: 0.5804\n",
      "Epoch 27/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 30.6466 - classification_subject_loss: 11.7306 - classification_object_loss: 11.7437 - classification_relationship_loss: 7.1723 - classification_subject_acc: 0.4127 - classification_object_acc: 0.4089 - classification_relationship_acc: 0.5416 - val_loss: 28.6341 - val_classification_subject_loss: 11.0568 - val_classification_object_loss: 11.0629 - val_classification_relationship_loss: 6.5144 - val_classification_subject_acc: 0.4404 - val_classification_object_acc: 0.4380 - val_classification_relationship_acc: 0.5817\n",
      "Epoch 28/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 30.4394 - classification_subject_loss: 11.6563 - classification_object_loss: 11.6524 - classification_relationship_loss: 7.1308 - classification_subject_acc: 0.4135 - classification_object_acc: 0.4139 - classification_relationship_acc: 0.5427 - val_loss: 28.5542 - val_classification_subject_loss: 11.0307 - val_classification_object_loss: 11.0375 - val_classification_relationship_loss: 6.4859 - val_classification_subject_acc: 0.4449 - val_classification_object_acc: 0.4406 - val_classification_relationship_acc: 0.5816\n",
      "Epoch 29/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 30.2657 - classification_subject_loss: 11.6027 - classification_object_loss: 11.5972 - classification_relationship_loss: 7.0658 - classification_subject_acc: 0.4149 - classification_object_acc: 0.4138 - classification_relationship_acc: 0.5458 - val_loss: 28.5028 - val_classification_subject_loss: 11.0092 - val_classification_object_loss: 11.0156 - val_classification_relationship_loss: 6.4780 - val_classification_subject_acc: 0.4427 - val_classification_object_acc: 0.4409 - val_classification_relationship_acc: 0.5834\n",
      "Epoch 30/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 30.0032 - classification_subject_loss: 11.4700 - classification_object_loss: 11.4926 - classification_relationship_loss: 7.0405 - classification_subject_acc: 0.4207 - classification_object_acc: 0.4194 - classification_relationship_acc: 0.5497 - val_loss: 28.3989 - val_classification_subject_loss: 10.9707 - val_classification_object_loss: 10.9780 - val_classification_relationship_loss: 6.4502 - val_classification_subject_acc: 0.4437 - val_classification_object_acc: 0.4429 - val_classification_relationship_acc: 0.5862\n",
      "Epoch 31/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 29.8513 - classification_subject_loss: 11.4218 - classification_object_loss: 11.4258 - classification_relationship_loss: 7.0038 - classification_subject_acc: 0.4206 - classification_object_acc: 0.4187 - classification_relationship_acc: 0.5517 - val_loss: 28.3733 - val_classification_subject_loss: 10.9670 - val_classification_object_loss: 10.9730 - val_classification_relationship_loss: 6.4332 - val_classification_subject_acc: 0.4449 - val_classification_object_acc: 0.4420 - val_classification_relationship_acc: 0.5869\n",
      "Epoch 32/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 29.6394 - classification_subject_loss: 11.3431 - classification_object_loss: 11.3502 - classification_relationship_loss: 6.9461 - classification_subject_acc: 0.4245 - classification_object_acc: 0.4236 - classification_relationship_acc: 0.5553 - val_loss: 28.3245 - val_classification_subject_loss: 10.9503 - val_classification_object_loss: 10.9563 - val_classification_relationship_loss: 6.4179 - val_classification_subject_acc: 0.4455 - val_classification_object_acc: 0.4422 - val_classification_relationship_acc: 0.5873\n",
      "Epoch 33/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 29.4396 - classification_subject_loss: 11.2676 - classification_object_loss: 11.2551 - classification_relationship_loss: 6.9169 - classification_subject_acc: 0.4280 - classification_object_acc: 0.4293 - classification_relationship_acc: 0.5578 - val_loss: 28.2222 - val_classification_subject_loss: 10.9098 - val_classification_object_loss: 10.9149 - val_classification_relationship_loss: 6.3975 - val_classification_subject_acc: 0.4478 - val_classification_object_acc: 0.4447 - val_classification_relationship_acc: 0.5890\n",
      "Epoch 34/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 29.2950 - classification_subject_loss: 11.2082 - classification_object_loss: 11.2084 - classification_relationship_loss: 6.8784 - classification_subject_acc: 0.4304 - classification_object_acc: 0.4292 - classification_relationship_acc: 0.5594 - val_loss: 28.1596 - val_classification_subject_loss: 10.8887 - val_classification_object_loss: 10.8894 - val_classification_relationship_loss: 6.3816 - val_classification_subject_acc: 0.4485 - val_classification_object_acc: 0.4444 - val_classification_relationship_acc: 0.5900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 29.1305 - classification_subject_loss: 11.1306 - classification_object_loss: 11.1378 - classification_relationship_loss: 6.8621 - classification_subject_acc: 0.4329 - classification_object_acc: 0.4339 - classification_relationship_acc: 0.5618 - val_loss: 28.1544 - val_classification_subject_loss: 10.8856 - val_classification_object_loss: 10.8904 - val_classification_relationship_loss: 6.3784 - val_classification_subject_acc: 0.4478 - val_classification_object_acc: 0.4442 - val_classification_relationship_acc: 0.5886\n",
      "Epoch 36/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 28.9969 - classification_subject_loss: 11.0853 - classification_object_loss: 11.0972 - classification_relationship_loss: 6.8145 - classification_subject_acc: 0.4346 - classification_object_acc: 0.4345 - classification_relationship_acc: 0.5658 - val_loss: 28.0859 - val_classification_subject_loss: 10.8653 - val_classification_object_loss: 10.8698 - val_classification_relationship_loss: 6.3509 - val_classification_subject_acc: 0.4494 - val_classification_object_acc: 0.4458 - val_classification_relationship_acc: 0.5917\n",
      "Epoch 37/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 28.7188 - classification_subject_loss: 10.9632 - classification_object_loss: 10.9708 - classification_relationship_loss: 6.7847 - classification_subject_acc: 0.4408 - classification_object_acc: 0.4390 - classification_relationship_acc: 0.5653 - val_loss: 28.0146 - val_classification_subject_loss: 10.8349 - val_classification_object_loss: 10.8425 - val_classification_relationship_loss: 6.3372 - val_classification_subject_acc: 0.4499 - val_classification_object_acc: 0.4460 - val_classification_relationship_acc: 0.5906\n",
      "Epoch 38/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 28.6531 - classification_subject_loss: 10.9482 - classification_object_loss: 10.9400 - classification_relationship_loss: 6.7648 - classification_subject_acc: 0.4415 - classification_object_acc: 0.4419 - classification_relationship_acc: 0.5675 - val_loss: 27.9868 - val_classification_subject_loss: 10.8266 - val_classification_object_loss: 10.8373 - val_classification_relationship_loss: 6.3228 - val_classification_subject_acc: 0.4536 - val_classification_object_acc: 0.4458 - val_classification_relationship_acc: 0.5910\n",
      "Epoch 39/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 28.4685 - classification_subject_loss: 10.8513 - classification_object_loss: 10.8666 - classification_relationship_loss: 6.7506 - classification_subject_acc: 0.4419 - classification_object_acc: 0.4421 - classification_relationship_acc: 0.5701 - val_loss: 27.9316 - val_classification_subject_loss: 10.8066 - val_classification_object_loss: 10.8187 - val_classification_relationship_loss: 6.3064 - val_classification_subject_acc: 0.4510 - val_classification_object_acc: 0.4497 - val_classification_relationship_acc: 0.5941\n",
      "Epoch 40/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 28.3447 - classification_subject_loss: 10.8091 - classification_object_loss: 10.8233 - classification_relationship_loss: 6.7123 - classification_subject_acc: 0.4485 - classification_object_acc: 0.4443 - classification_relationship_acc: 0.5695 - val_loss: 27.8811 - val_classification_subject_loss: 10.7902 - val_classification_object_loss: 10.7992 - val_classification_relationship_loss: 6.2917 - val_classification_subject_acc: 0.4526 - val_classification_object_acc: 0.4498 - val_classification_relationship_acc: 0.5937\n",
      "Epoch 41/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 28.1869 - classification_subject_loss: 10.7613 - classification_object_loss: 10.7474 - classification_relationship_loss: 6.6782 - classification_subject_acc: 0.4468 - classification_object_acc: 0.4471 - classification_relationship_acc: 0.5742 - val_loss: 27.8701 - val_classification_subject_loss: 10.7826 - val_classification_object_loss: 10.7953 - val_classification_relationship_loss: 6.2923 - val_classification_subject_acc: 0.4526 - val_classification_object_acc: 0.4497 - val_classification_relationship_acc: 0.5947\n",
      "Epoch 42/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 28.0694 - classification_subject_loss: 10.6864 - classification_object_loss: 10.7042 - classification_relationship_loss: 6.6788 - classification_subject_acc: 0.4504 - classification_object_acc: 0.4487 - classification_relationship_acc: 0.5751 - val_loss: 27.8320 - val_classification_subject_loss: 10.7705 - val_classification_object_loss: 10.7812 - val_classification_relationship_loss: 6.2802 - val_classification_subject_acc: 0.4535 - val_classification_object_acc: 0.4504 - val_classification_relationship_acc: 0.5937\n",
      "Epoch 43/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 28.0079 - classification_subject_loss: 10.6823 - classification_object_loss: 10.6866 - classification_relationship_loss: 6.6389 - classification_subject_acc: 0.4511 - classification_object_acc: 0.4480 - classification_relationship_acc: 0.5759 - val_loss: 27.7626 - val_classification_subject_loss: 10.7438 - val_classification_object_loss: 10.7521 - val_classification_relationship_loss: 6.2668 - val_classification_subject_acc: 0.4532 - val_classification_object_acc: 0.4505 - val_classification_relationship_acc: 0.5946\n",
      "Epoch 44/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 27.7764 - classification_subject_loss: 10.6113 - classification_object_loss: 10.5708 - classification_relationship_loss: 6.5943 - classification_subject_acc: 0.4523 - classification_object_acc: 0.4538 - classification_relationship_acc: 0.5791 - val_loss: 27.7438 - val_classification_subject_loss: 10.7406 - val_classification_object_loss: 10.7504 - val_classification_relationship_loss: 6.2528 - val_classification_subject_acc: 0.4530 - val_classification_object_acc: 0.4507 - val_classification_relationship_acc: 0.5959\n",
      "Epoch 45/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 27.6351 - classification_subject_loss: 10.5182 - classification_object_loss: 10.5374 - classification_relationship_loss: 6.5796 - classification_subject_acc: 0.4574 - classification_object_acc: 0.4562 - classification_relationship_acc: 0.5804 - val_loss: 27.6895 - val_classification_subject_loss: 10.7214 - val_classification_object_loss: 10.7316 - val_classification_relationship_loss: 6.2365 - val_classification_subject_acc: 0.4548 - val_classification_object_acc: 0.4505 - val_classification_relationship_acc: 0.5983\n",
      "Epoch 46/50\n",
      "61860/61860 [==============================] - 26s 419us/step - loss: 27.5399 - classification_subject_loss: 10.4808 - classification_object_loss: 10.4884 - classification_relationship_loss: 6.5708 - classification_subject_acc: 0.4591 - classification_object_acc: 0.4574 - classification_relationship_acc: 0.5818 - val_loss: 27.6264 - val_classification_subject_loss: 10.6954 - val_classification_object_loss: 10.7017 - val_classification_relationship_loss: 6.2293 - val_classification_subject_acc: 0.4533 - val_classification_object_acc: 0.4514 - val_classification_relationship_acc: 0.5981\n",
      "Epoch 47/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 27.3574 - classification_subject_loss: 10.4169 - classification_object_loss: 10.4278 - classification_relationship_loss: 6.5127 - classification_subject_acc: 0.4594 - classification_object_acc: 0.4622 - classification_relationship_acc: 0.5849 - val_loss: 27.6413 - val_classification_subject_loss: 10.7031 - val_classification_object_loss: 10.7080 - val_classification_relationship_loss: 6.2303 - val_classification_subject_acc: 0.4546 - val_classification_object_acc: 0.4520 - val_classification_relationship_acc: 0.5988\n",
      "Epoch 48/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 27.2764 - classification_subject_loss: 10.3719 - classification_object_loss: 10.3896 - classification_relationship_loss: 6.5150 - classification_subject_acc: 0.4634 - classification_object_acc: 0.4625 - classification_relationship_acc: 0.5853 - val_loss: 27.5671 - val_classification_subject_loss: 10.6685 - val_classification_object_loss: 10.6832 - val_classification_relationship_loss: 6.2153 - val_classification_subject_acc: 0.4575 - val_classification_object_acc: 0.4544 - val_classification_relationship_acc: 0.5975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "61860/61860 [==============================] - 26s 418us/step - loss: 27.1854 - classification_subject_loss: 10.3262 - classification_object_loss: 10.3423 - classification_relationship_loss: 6.5169 - classification_subject_acc: 0.4646 - classification_object_acc: 0.4640 - classification_relationship_acc: 0.5871 - val_loss: 27.5561 - val_classification_subject_loss: 10.6691 - val_classification_object_loss: 10.6826 - val_classification_relationship_loss: 6.2044 - val_classification_subject_acc: 0.4542 - val_classification_object_acc: 0.4526 - val_classification_relationship_acc: 0.5999\n",
      "Epoch 50/50\n",
      "61860/61860 [==============================] - 26s 417us/step - loss: 27.0604 - classification_subject_loss: 10.2777 - classification_object_loss: 10.2941 - classification_relationship_loss: 6.4885 - classification_subject_acc: 0.4677 - classification_object_acc: 0.4658 - classification_relationship_acc: 0.5870 - val_loss: 27.5340 - val_classification_subject_loss: 10.6612 - val_classification_object_loss: 10.6701 - val_classification_relationship_loss: 6.2027 - val_classification_subject_acc: 0.4574 - val_classification_object_acc: 0.4548 - val_classification_relationship_acc: 0.6008\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "print (\"Input shape: {}\".format(X_train[0].shape))\n",
    "disable_regress = True\n",
    "model = DeepClassifyRegressModel(X_train[0].shape, subject_output_class_size=len(mlb_subject_object.classes_),\n",
    "                                 object_output_class_size=len(mlb_subject_object.classes_), \n",
    "                                 relationship_output_class_size=len(mlb_relationship.classes_),\n",
    "                                 output_reg_size=8,\n",
    "                                 disable_regress=disable_regress)\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "if disable_regress:\n",
    "    y_train_final = y_train[1:]   # Remove the regression y data.\n",
    "\n",
    "model_history = train_model(model, X_train, y_train_final, epochs=epochs, batch_size=batch_size, \n",
    "                            disable_regress=disable_regress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "Save the model that has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the weights.\n",
    "model.save_weights('stage2_model_epoch_10_weights.h5')\n",
    "model.save('stage2_model_epoch_10.h5')\n",
    "\n",
    "# Save the model architecture.\n",
    "with open('stage2_model.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "    \n",
    "# Save the MLB labels for later use.\n",
    "with open(\"mlb_subject_object.p\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(mlb_subject_object))\n",
    "    \n",
    "with open(\"mlb_relationship.p\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(mlb_relationship))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History keys: dict_keys(['val_classification_object_loss', 'classification_object_loss', 'classification_object_acc', 'val_classification_subject_acc', 'classification_subject_loss', 'loss', 'classification_relationship_loss', 'val_classification_object_acc', 'classification_relationship_acc', 'val_classification_relationship_loss', 'val_classification_relationship_acc', 'classification_subject_acc', 'val_loss', 'val_classification_subject_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYRGmoIQQgiNNAUhhBAaaQpCCCE0/wDJbHMGOxQGnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc732787710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "print ('History keys: {}'.format(model_history.history.keys()))\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs), model_history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, epochs), model_history.history[\"val_loss\"], label=\"val_loss\")\n",
    "#plt.plot(np.arange(0, epochs), model_history.history[\"val_regression_loss\"], label=\"val_regression_loss\")\n",
    "#plt.plot(np.arange(0, epochs), model_history.history[\"val_classification_loss\"], label=\"val_classification_loss\")\n",
    "#plt.plot(np.arange(0, epochs), model_history.history[\"classification_acc\"], label=\"classification_acc\")\n",
    "#plt.plot(np.arange(0, epochs), model_history.history[\"val_classification_acc\"], label=\"val_classification_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(\"model_training_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Load the saved model.\n",
    "saved_model = 'stage2_model_epoch_10.h5'\n",
    "saved_model_weights = 'stage2_model_epoch_10_weights.h5'\n",
    "json_model = 'stage2_model.json'\n",
    "infer_model = load_model(saved_model, custom_objects={'PriorProbability': PriorProbability, \n",
    "                                                      'focal_loss_fixed': focal_loss(),\n",
    "                                                      'softmax': tf.nn.softmax})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time:  0.4889090061187744\n",
      "Prediction : [array([[ 0.00026348,  0.00203103,  0.0477102 ,  0.00048003,  0.00044745,\n",
      "         0.00243155,  0.03770346,  0.00041694,  0.08572096,  0.01437806,\n",
      "         0.0140356 ,  0.00241337,  0.01281908,  0.00044907,  0.00080647,\n",
      "         0.00149425,  0.0016595 ,  0.0598856 ,  0.16639811,  0.00026362,\n",
      "         0.02139124,  0.00040773,  0.00341866,  0.00083225,  0.06495046,\n",
      "         0.00114342,  0.02915228,  0.00034747,  0.20556101,  0.00063821,\n",
      "         0.00230344,  0.00030429,  0.0007557 ,  0.01417188,  0.04048564,\n",
      "         0.00032792,  0.00041095,  0.01420562,  0.00093084,  0.00881992,\n",
      "         0.02660077,  0.04190428,  0.00042424,  0.00033706,  0.00338042,\n",
      "         0.00375057,  0.00025762,  0.0014349 ,  0.05080274,  0.00040886,\n",
      "         0.00239472,  0.00071445,  0.0003428 ,  0.00046949,  0.00107384,\n",
      "         0.00052478,  0.00082004,  0.00052548,  0.00048308,  0.0006607 ,\n",
      "         0.0003224 ]], dtype=float32), array([[ 0.00042171,  0.00157569,  0.04653739,  0.00042294,  0.00045817,\n",
      "         0.00377701,  0.03023799,  0.00037146,  0.09760901,  0.02333182,\n",
      "         0.01763378,  0.00221925,  0.01194609,  0.00141998,  0.00046438,\n",
      "         0.00189455,  0.00137793,  0.05409164,  0.16080537,  0.00062428,\n",
      "         0.0155259 ,  0.00115958,  0.00306425,  0.00053366,  0.06715555,\n",
      "         0.00105619,  0.03683722,  0.00055991,  0.18127006,  0.00119138,\n",
      "         0.00350519,  0.00063087,  0.00070433,  0.01850243,  0.04072678,\n",
      "         0.00075357,  0.00047726,  0.02238144,  0.00036214,  0.0049427 ,\n",
      "         0.02934369,  0.03928937,  0.00037175,  0.00052336,  0.00232338,\n",
      "         0.00535574,  0.00053072,  0.00168936,  0.05367977,  0.00042998,\n",
      "         0.0021843 ,  0.00049941,  0.00078033,  0.00063748,  0.00056383,\n",
      "         0.00070198,  0.00045618,  0.00073834,  0.00061038,  0.00028015,\n",
      "         0.00044969]], dtype=float32), array([[ 0.0668663 ,  0.03246867,  0.31338418,  0.03111947,  0.05100887,\n",
      "         0.30876976,  0.13862836,  0.03789989,  0.00205229,  0.01780225]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "prediction = infer_model.predict_on_batch(np.expand_dims(X_test[0], axis=0))\n",
    "print(\"processing time: \", time.time() - start)\n",
    "print ('Prediction : {}'.format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model prediction to labels for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject label prediction: ([-1, -1, -1, -1, -1, -1, -1, -1], ('/m/04yx4', '/m/04yx4', 'holds'), 0.23340508341789246)\n"
     ]
    }
   ],
   "source": [
    "def get_score(subject_prediction, object_prediction, relationship_prediction, disable_regress=True):\n",
    "    subject_max = float(np.amax(subject_prediction))\n",
    "    object_max = float(np.amax(object_prediction))\n",
    "    relationship_max = float(np.amax(relationship_prediction))\n",
    "    return (subject_max + object_max + relationship_max)/3.0\n",
    "\n",
    "def get_label(lb, softmax_vector):\n",
    "    a = np.asarray(softmax_vector).reshape(-1)\n",
    "    b = np.zeros_like(a)\n",
    "    b[np.argmax(a)] = 1\n",
    "    return lb.inverse_transform(b.reshape(1, -1))[0]\n",
    "\n",
    "def convert_prediction_to_label(lb_subject_object, lb_relationship_object, prediction, disable_regress=True):\n",
    "    # regress output\n",
    "    start_idx = 0\n",
    "    if disable_regress is False:\n",
    "        boxes = prediction[start_idx]\n",
    "        start_idx +=1\n",
    "\n",
    "    label_subject_probs = prediction[start_idx]\n",
    "    label_subject = get_label(lb_subject_object, prediction[start_idx])\n",
    "    label_object = get_label(lb_subject_object, prediction[start_idx + 1])\n",
    "    label_relationship = get_label(lb_relationship_object, prediction[start_idx + 2])\n",
    "    score = get_score(prediction[start_idx], prediction[start_idx + 1], prediction[start_idx + 2])\n",
    "    if disable_regress is False:\n",
    "        return (boxes.reshape(-1).tolist(), (label_subject, label_object, label_relationship), score)\n",
    "    else:\n",
    "        return ([-1]*8, (label_subject, label_object, label_relationship), score)\n",
    "\n",
    "print ('Subject label prediction: {}'.format(convert_prediction_to_label(mlb_subject_object,                                                 \n",
    "                                                                         mlb_relationship,\n",
    "                                                                         prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (16645, 600)\n",
      "processing time:  0.9601843357086182\n"
     ]
    }
   ],
   "source": [
    "# Get prediction for test set to evaluate score.\n",
    "print ('X_test shape: {}'.format(X_test.shape))\n",
    "start = time.time()\n",
    "predictions = infer_model.predict(X_test)\n",
    "print(\"processing time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction[0] shape: (16645, 61), prediction[1] shape: (16645, 61), prediction[2] shape: (16645, 10)\n"
     ]
    }
   ],
   "source": [
    "print ('Prediction[0] shape: {}, prediction[1] shape: {}, prediction[2] shape: {}'.format(\n",
    "    predictions[0].shape, predictions[1].shape, predictions[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entry_for_eval(fid, predictions, lb_subject_object, lb_relationship_object, disable_regress=True):\n",
    "    boxes_labels_tuple = convert_prediction_to_label(lb_subject_object, lb_relationship_object, predictions,\n",
    "                                                    disable_regress=disable_regress)\n",
    "    return [fid, boxes_labels_tuple[1][0], boxes_labels_tuple[1][1]] + boxes_labels_tuple[0] + \\\n",
    "            [boxes_labels_tuple[1][2]] + [boxes_labels_tuple[2]]\n",
    "\n",
    "def process_test_set_predictions(test_dataset, X_test, infer_model, \n",
    "                                 lb_subject_object, lb_relationship_object,\n",
    "                                 disable_regress=True):\n",
    "    print ('X_test shape: {}'.format(X_test.shape))\n",
    "    start = time.time()\n",
    "    predictions = infer_model.predict(X_test)\n",
    "    print(\"processing time: \", time.time() - start)\n",
    "    ret_list = []\n",
    "    print ('processing entry count: {}'.format(len(test_dataset['id'])))\n",
    "    for i in range(0, len(test_dataset['id'])):\n",
    "        fid = test_dataset['id'][i]\n",
    "        if disable_regress is False:\n",
    "            eval_output = get_entry_for_eval(fid, \n",
    "                                             [predictions[0][i],\n",
    "                                              predictions[1][i].reshape(1, -1), \n",
    "                                              predictions[2][i].reshape(1, -1), \n",
    "                                              predictions[3][i].reshape(1, -1)], lb_subject_object, \n",
    "                                              lb_relationship_object)\n",
    "            ret_list.append(eval_output)\n",
    "        else:\n",
    "            eval_output = get_entry_for_eval(fid, \n",
    "                                             [predictions[0][i].reshape(1, -1), \n",
    "                                              predictions[1][i].reshape(1, -1), \n",
    "                                              predictions[2][i].reshape(1, -1)], lb_subject_object, \n",
    "                                              lb_relationship_object, disable_regress=disable_regress)\n",
    "            ret_list.append(eval_output)\n",
    "\n",
    "    return ret_list\n",
    "\n",
    "def ouput_validation_csv(validation_list, csv_file, prefix='validation', disable_regress=True):\n",
    "    csv_file_path = os.path.join(prefix, csv_file)\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf8') as f:\n",
    "        write_one_extra = False\n",
    "        writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(\n",
    "    \"ImageID,LabelName1,LabelName2,XMin1,XMax1,YMin1,YMax1,XMin2,XMax2,YMin2,YMax2,RelationshipLabel,Score\".split(','))\n",
    "        for entry in validation_list:\n",
    "            writer.writerow(entry)\n",
    "            if write_one_extra is False:\n",
    "                writer.writerow(entry)  # Hack to write twice to fix error in ooid_vrd_challenge_evaluation.py\n",
    "                write_one_extra = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (16645, 600)\n",
      "processing time:  0.9924373626708984\n",
      "processing entry count: 16645\n"
     ]
    }
   ],
   "source": [
    "disable_regress=True\n",
    "validation_list = process_test_set_predictions(model_test_dataset, X_test, infer_model, \n",
    "                                               mlb_subject_object, mlb_relationship, \n",
    "                                               disable_regress=disable_regress)\n",
    "ouput_validation_csv(validation_list, csv_file='stage2_model_epoch_10_testset_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter input csv for evalulation\n",
    "\n",
    "This is mentioned here:\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/challenge_evaluation.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_ground_truth_csv(prefix='data/raw', train_csv_fname = 'challenge-2018-train-vrd.csv', \n",
    "                            test_csv_fname = 'validation/stage2_model_epoch_10_testset_val.csv',\n",
    "                            filtered_csv_fname = 'validation/filtered-challenge-2018-train-vrd.csv'):\n",
    "    \n",
    "    test_fid_set = set()\n",
    "    with open(test_csv_fname, mode='r', encoding='utf8') as f:\n",
    "        rows = csv.reader(f)\n",
    "        for row in rows:\n",
    "            test_fid_set.add(row[0])\n",
    "    \n",
    "    filtered_truth_rows = []\n",
    "    with open(os.path.join(prefix, train_csv_fname), mode='r', encoding='utf8') as f:\n",
    "        rows = csv.reader(f)\n",
    "        for row in rows:\n",
    "            if row[0] in test_fid_set:\n",
    "                filtered_truth_rows.append(row)\n",
    "        \n",
    "    with open(filtered_csv_fname, mode='w', newline='', encoding='utf8') as f:\n",
    "        writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for entry in filtered_truth_rows:\n",
    "            writer.writerow(entry)\n",
    "            \n",
    "    return filtered_truth_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered set size: 109699\n"
     ]
    }
   ],
   "source": [
    "training_csv = 'challenge-2018-train-vrd.csv'\n",
    "validation_csv = 'validation/stage2_model_epoch_10_testset_val.csv'\n",
    "filtered_csv_fname = 'validation/filtered-challenge-2018-train-vrd.csv'\n",
    "filtered_ground_truth_rows = filter_ground_truth_csv(prefix='data/raw', \n",
    "                                                     train_csv_fname = 'challenge-2018-train-vrd.csv', \n",
    "                                                     test_csv_fname = validation_csv,\n",
    "                                                     filtered_csv_fname = filtered_csv_fname)\n",
    "print ('Filtered set size: {}'.format(len(filtered_ground_truth_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers for mAP calculation without using IOU for now\n",
    "\n",
    "Only measure triplet detection accuracy per class and across all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_triplet_to_image_id_dict_from_csv(csv_fname):\n",
    "    ret_dict = {}\n",
    "    \n",
    "    with open(csv_fname, mode='r', encoding='utf8') as f:\n",
    "        rows = csv.reader(f)\n",
    "        header_process = False\n",
    "        relationship_idx = -1\n",
    "        for row in rows:\n",
    "            if header_process is False:\n",
    "                print ('header for {}: {}'.format(csv_fname, row))\n",
    "                relationship_idx = row.index('RelationshipLabel')\n",
    "                header_process = True\n",
    "                continue\n",
    "            fid = row[0]\n",
    "            subject_class = row[1]\n",
    "            object_class = row[2]\n",
    "            relationship_class = row[relationship_idx]\n",
    "            boxes = row[3:11]\n",
    "            k = (subject_class, object_class, relationship_class)\n",
    "            if k in ret_dict:\n",
    "                if fid in ret_dict[k]:\n",
    "                    ret_dict[k][fid].append(boxes)\n",
    "                else:\n",
    "                    ret_dict[k][fid] = [boxes]\n",
    "            else:\n",
    "                ret_dict[k] = {fid: [boxes]}\n",
    "    return ret_dict\n",
    "\n",
    "def get_triplet_to_image_id_dict_from_data_set(model_data_input):\n",
    "    \"\"\"\n",
    "    When not considering bounding box data dimension for train and test set we only have a \n",
    "    constrained set of triplets. Get them for evaluation.\n",
    "    \"\"\"\n",
    "    ret_dict = {}\n",
    "\n",
    "    for fid, boxes, labels_orig in zip(model_data_input['id'], model_data_input['boxes'], \n",
    "                                       model_data_input['labels_orig']):\n",
    "        k = labels_orig\n",
    "        if k not in ret_dict:\n",
    "            ret_dict[k] = {}\n",
    "\n",
    "        if fid not in ret_dict[k]:\n",
    "            ret_dict[k][fid] = []\n",
    "  \n",
    "        ret_dict[k][fid].append(boxes)\n",
    "\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header for validation/stage2_model_epoch_10_testset_val.csv: ['ImageID', 'LabelName1', 'LabelName2', 'XMin1', 'XMax1', 'YMin1', 'YMax1', 'XMin2', 'XMax2', 'YMin2', 'YMax2', 'RelationshipLabel', 'Score']\n"
     ]
    }
   ],
   "source": [
    "ground_truth_triplet_dict = get_triplet_to_image_id_dict_from_data_set(model_data_input=model_test_dataset)\n",
    "model_test_triplet_dict, model_test_rel_dict = get_triplets_from_dataset(model_test_dataset)\n",
    "testset_triplet_dict = get_triplet_to_image_id_dict_from_csv(csv_fname=validation_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model test triplet keys length: 231\n",
      "testset triplet_keys lenth: 163\n"
     ]
    }
   ],
   "source": [
    "print ('model test triplet keys length: {}'.format(len(ground_truth_triplet_dict.keys())))\n",
    "print ('testset triplet_keys lenth: {}'.format(len(testset_triplet_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_per_class_precision_recall_dict(ground_truth_triplet_dict, testset_triplet_dict):\n",
    "    class_dict = {}\n",
    "    for triplet, v in testset_triplet_dict.items():\n",
    "        #print('Triplet: {}'.format(triplet))\n",
    "        detection_count = len(v.keys())        \n",
    "        true_positives = 0.0\n",
    "        ground_truth_count = 0.0\n",
    "        if triplet in ground_truth_triplet_dict:\n",
    "            result_fid_set = set(v.keys())\n",
    "            ground_fid_set = set(ground_truth_triplet_dict[triplet].keys())\n",
    "            ground_truth_count  = len(ground_fid_set)\n",
    "            true_positives = len(result_fid_set.intersection(ground_fid_set))\n",
    "        #print ('+ive: {}, detection_count: {}'.format(true_positives, detection_count))\n",
    "        precision = float(true_positives)/float(detection_count)\n",
    "        if ground_truth_count != 0.0:\n",
    "            recall = float(true_positives)/float(ground_truth_count)\n",
    "        else:\n",
    "            recall = None  # Invalid triplet!.\n",
    "        class_dict[triplet] = (precision, recall)\n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triplet_precision_recall = get_per_class_precision_recall_dict(ground_truth_triplet_dict, testset_triplet_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing label count: 0\n"
     ]
    }
   ],
   "source": [
    "xy_list, train_data_label_tuple, label_dict = process_raw_csv_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per triplet precision and recall\n",
    "\n",
    "List the precision and recall obtained per triplet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woman-at-Table:0.44,0.010309\n",
      "Man-holds-Guitar:0.0,0.0\n",
      "Man-holds-Bottle:0.0,0.0\n",
      "Man-at-Table:0.26087,0.004428\n",
      "Chair-at-Table:0.083333,0.000645\n",
      "Man-on-Chair:0.0,0.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in triplet_precision_recall.items():\n",
    "    #print ('Triplet: {}'.format(k))\n",
    "    if v[1] is not None:\n",
    "        print ('{}-{}-{}:{},{}'.format(label_dict[k[0]], k[2], label_dict[k[1]], \n",
    "            round(float(v[0]), 6), round(float(v[1]), 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
