{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images Visual Relationship project\n",
    "This project is based on the [Google AI Open Images - Visual Relationship Track Kaggle Challenge](https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track).\n",
    "\n",
    "The challenge is to build the best performing algorithm for automatically detecting relationships triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using COCO trained weights: resnet50_coco_best_v2.1.0.h5 from path: /home/powell/work/scpd/vision_project/visual_relationship/pretrained_models/resnet50_coco_best_v2.1.0.h5, downloaded from: https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import wget\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import collections\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import __version__ as keras_version\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Cropping2D, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# https://github.com/fizyr/keras-retinanet/blob/master/examples/ResNet50RetinaNet.ipynb\n",
    "import keras_retinanet\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "#import tensorflow as tf\n",
    "\n",
    "#def get_session():\n",
    "#    config = tf.ConfigProto()\n",
    "#    config.gpu_options.allow_growth = True\n",
    "#    return tf.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "#keras.backend.tensorflow_backend.set_session(get_session())\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5\n",
    "RCNN_COCO_MODEL_URL = \"https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5\"\n",
    "RCNN_COCO_MODEL = \"resnet50_coco_best_v2.1.0.h5\"\n",
    "\n",
    "# Local path to trained weights file\n",
    "MODEL_PATH = os.path.join(os.path.join(os.getcwd(), \"pretrained_models\"), RCNN_COCO_MODEL)\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print ('Downloading COCO trained weights: {} to path: {}'.format(RCNN_COCO_MODEL, MODEL_PATH))  \n",
    "    wget.download(RCNN_COCO_MODEL_URL, MODEL_PATH)  \n",
    "else:\n",
    "    print ('Using COCO trained weights: {} from path: {}, downloaded from: {}'.format(\n",
    "        RCNN_COCO_MODEL, MODEL_PATH, RCNN_COCO_MODEL_URL))\n",
    "    \n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper API\n",
    "\"\"\"\n",
    "def process_labels_from_csv_input(prefix='data/raw', \n",
    "                                  labels_csv_fname_list=['class-descriptions-boxable.csv', \n",
    "                                                         'class-descriptions.csv']):\n",
    "    label_dict = {}\n",
    "    # Process the labels info files and create a key value pair\n",
    "    for flabel_csv in labels_csv_fname_list:\n",
    "        with open(os.path.join(prefix, flabel_csv)) as f:\n",
    "            rows = csv.reader(f)\n",
    "            for row in rows:\n",
    "                if row[0] in label_dict:\n",
    "                    #print('Label already exists: {}:{}, new label: {}:{}'.\n",
    "                    #       format(row[0], label_dict[row[0]], row[0], row[1]))\n",
    "                    assert row[1] == label_dict[row[0]]\n",
    "                label_dict[row[0]]=row[1]\n",
    "                \n",
    "    return label_dict\n",
    "                \n",
    "def process_raw_csv_input(prefix='data/raw', train_csv_fname = 'challenge-2018-train-vrd.csv', \n",
    "                          labels_csv_fname_list = ['class-descriptions-boxable.csv', 'class-descriptions.csv']):\n",
    "    \"\"\"\n",
    "    Process the labels from given label names and create three categories\n",
    "    a. Entity\n",
    "    b. Attribute\n",
    "    c. Relationship\n",
    "    \"\"\"\n",
    "    \n",
    "    label_dict = process_labels_from_csv_input(prefix, labels_csv_fname_list)\n",
    "    \n",
    "    # Process the training data and create x, y i.e x->(imageid, (bounding box data)) y->(label1,label2,relationship)\n",
    "    label1_dict = collections.defaultdict(int)\n",
    "    label2_dict = collections.defaultdict(int)\n",
    "    relationship_dict = collections.defaultdict(int)\n",
    "    xy_list = []\n",
    "    missing_label_dict = {}\n",
    "    ignore_header = True\n",
    "    miss_count = 0\n",
    "    with open(os.path.join(prefix, train_csv_fname)) as f:\n",
    "        rows = csv.reader(f)\n",
    "        for row in rows:\n",
    "            miss = False\n",
    "            if ignore_header:\n",
    "                ignore_header = False\n",
    "                continue\n",
    "            x = (row[0], (row[3:11]))\n",
    "            y = (row[1], row[2], row[11])\n",
    "            if y[0] not in label_dict:\n",
    "                if y[0] not in missing_label_dict:\n",
    "                    print('Label1 missing: {}'.format(y[0]))\n",
    "                miss = True\n",
    "                missing_label_dict[y[0]] = y[0]\n",
    "            else:\n",
    "                label1_dict[y[0]] += 1\n",
    "            if y[1] not in label_dict:\n",
    "                if y[1] not in missing_label_dict:\n",
    "                    print('Label2 missing: {}, label1 : {}, relation: {}'.format(y[1], label_dict[y[0]], row[11]))\n",
    "                miss_count += 1\n",
    "                miss = True\n",
    "                missing_label_dict[y[1]] = y[1]\n",
    "            else:\n",
    "                label2_dict[y[1]] += 1\n",
    "            relationship_dict[y[2]] += 1\n",
    "            if miss is False:\n",
    "                xy_list.append((x, y))\n",
    "    print (\"Missing label count: {}\".format(miss_count))             \n",
    "    return xy_list, (label1_dict, label2_dict, relationship_dict), label_dict\n",
    "\n",
    "def get_data_dir_from_raw_single_dir(X_dict, prefix='data', dir_list=None, out_dir='processed'):\n",
    "    X_fset = set()\n",
    "    copy_prefix_dir = os.path.join(prefix, out_dir)\n",
    "    for d in dir_list:\n",
    "        copy_dir = os.path.join(os.getcwd(), os.path.join(copy_prefix_dir, d))\n",
    "        os.makedirs(copy_dir, exist_ok=True)\n",
    "        flist = glob.glob(os.path.join(os.path.join(prefix, d), '*.jpg'))\n",
    "        for f in flist:\n",
    "            fid = os.path.basename(f).split('.')[0]\n",
    "            if fid in X_dict:\n",
    "                dst_f = os.path.join(copy_dir, os.path.basename(f))\n",
    "                X_fset.add(dst_f)\n",
    "                X_dict[fid] = (X_dict[fid], dst_f)\n",
    "                copyfile(os.path.join(os.getcwd(), f), dst_f)\n",
    "                \n",
    "    return X_fset\n",
    "\n",
    "def get_data_from_dir_recursive(xy_list, prefix='data/processed', dir_input='raw'):\n",
    "    \"\"\"\n",
    "    Load the file path for each image id. The dictionary can only have image file path since\n",
    "    a single image can have multiple labels i.e multiple y values.\n",
    "\n",
    "    Example of entry in xy_list:\n",
    "    Train data xy_list[0]: (\n",
    "                            ('fe58ec1b06db2bb7', ['0.005', '0.033125', '0.58', '0.62777776', \n",
    "                                                   '0.005', '0.033125', '0.58', '0.62777776']) , \n",
    "                            ('/m/04bcr3', '/m/083vt', 'is'))\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    xy_list_valid = []      # xy_list that has valid image files available\n",
    "    X_id_to_file_dict = {}  # id of the image to file dictionary\n",
    "    def process_files(dir_path):\n",
    "        flist = glob.glob(os.path.join(dir_path, '*.jpg'))\n",
    "        print('Processing dir: {}, image count: {}'.format(dir_path, len(flist)))\n",
    "            \n",
    "        for f in flist:\n",
    "            fid = os.path.basename(f).split('.')[0]\n",
    "            if fid in X_id_to_file_dict:\n",
    "                print ('Error id exists twice: {}-{}-{}'.format(fid, f, X_id_to_file_dict[fid]))\n",
    "                continue\n",
    "            else:\n",
    "                X_id_to_file_dict[fid] = os.path.join(cwd, f)\n",
    "                \n",
    "    def helper(dir_input_full):\n",
    "        l = next(os.walk(dir_input_full))[1]\n",
    "        if len(l) == 0:   \n",
    "            return\n",
    "        \n",
    "        for d in l:\n",
    "            dir_path = os.path.join(dir_input_full, d)\n",
    "            process_files(dir_path)\n",
    "            helper(dir_path)\n",
    "    \n",
    "    process_files(os.path.join(prefix, dir_input))\n",
    "    helper(os.path.join(prefix, dir_input))\n",
    "    \n",
    "    for xy in xy_list:\n",
    "        if xy[0][0] in X_id_to_file_dict:\n",
    "            xy_list_valid.append(xy)\n",
    "\n",
    "    return xy_list_valid, X_id_to_file_dict\n",
    "\n",
    "def bounding_box_to_plt(image, b):\n",
    "    \"\"\"\n",
    "    Convert one bounding box data into what mathplotlib understands\n",
    "    [XMin1,    XMax1,     YMin1,   YMax1,        XMin2,    XMax2,    YMin2,   YMax2]\n",
    "    ['0.005', '0.033125', '0.58', '0.62777776', '0.005', '0.033125', '0.58', '0.62777776']\n",
    "    for: https://matplotlib.org/api/_as_gen/matplotlib.patches.Rectangle.html#matplotlib.patches.Rectangle\n",
    "    \"\"\"\n",
    "    xsize = image.shape[1]\n",
    "    ysize = image.shape[0]\n",
    "    xy = (int(float(b[0]) * xsize), int(float(b[2]) * ysize))   # (XMin1 * xsize, YMin1 * ysize)\n",
    "    width = int(float(b[1]) * xsize) - xy[0]        # XMax1 * xsize - XMin1 * xsize\n",
    "    height = int(float(b[3]) * ysize) - xy[1]       # YMax1 * ysize - Ymin * ysize \n",
    "    return (xy, width, height)\n",
    "\n",
    "def two_bounding_boxes_to_plt(image, b):\n",
    "    \"\"\"\n",
    "    Convert two bounding box data into what mathplotlib understands\n",
    "    \"\"\"\n",
    "    return [bounding_box_to_plt(image, b[0:4]), bounding_box_to_plt(image, b[4:len(b)])]\n",
    "    \n",
    "def show_images(images,titles=None, bounding_boxes_list=[]):\n",
    "    \"\"\"Display a list of images\"\"\"\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    \n",
    "    for i in range(0, len(images)):\n",
    "        image = images[i]\n",
    "        title = \"None\"\n",
    "        if titles is not None and len(titles) > i:\n",
    "            title = titles[i]\n",
    "        \n",
    "        bounding_boxes = None\n",
    "        if bounding_boxes_list is not None and len(bounding_boxes_list) > i:\n",
    "            bounding_boxes = bounding_boxes_list[i]\n",
    "\n",
    "        a = fig.add_subplot(1,n_ims,n) # Make subplot\n",
    "        if len(image.shape) == 2 or image.shape[2] == 1: # Is image grayscale?\n",
    "            plt.imshow(np.resize(image, (image.shape[0], image.shape[1])), interpolation=\"bicubic\", cmap=\"gray\") # Only place in this blog you can't replace 'gray' with 'grey'\n",
    "        else:\n",
    "            plt.imshow(image, interpolation=\"bicubic\")\n",
    "            if bounding_boxes is not None:\n",
    "                box1, box2 = two_bounding_boxes_to_plt(image, bounding_boxes)\n",
    "                rect1 = patches.Rectangle((box1[0]),box1[1],box1[2],linewidth=2,edgecolor='y',facecolor='none')\n",
    "                rect2 = patches.Rectangle((box2[0]),box2[1],box2[2],linewidth=2,edgecolor='g',facecolor='none')\n",
    "                a.add_patch(rect1)\n",
    "                a.add_patch(rect2)\n",
    "        if titles is not None:\n",
    "            a.set_title(title + ' {}x{}'.format(image.shape[0], image.shape[1]))\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def show_given_images(xy_given_list, id_to_file_dict):\n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    bounding_boxes_list = []\n",
    "    for xy in xy_given_list:\n",
    "        fid = xy[0][0]\n",
    "        bounding_boxes_list.append(xy[0][1])\n",
    "        y = xy[1]\n",
    "        label1 = y[0]\n",
    "        label2 = y[1]\n",
    "        if label1 in label_dict:\n",
    "            label1 = label_dict[label1]\n",
    "        if label2 in label_dict:\n",
    "            label2 = label_dict[label2]\n",
    "        \n",
    "        label_list.append('{} {} {}'.format(label1, y[2], label2))\n",
    "        if fid not in id_to_file_dict:\n",
    "            print ('Error could not find id: {} in id_to_file_dict'.format(fid))\n",
    "            raise \n",
    "        img_list.append(cv2.cvtColor(cv2.imread(id_to_file_dict[fid], cv2.IMREAD_COLOR), cv2.COLOR_RGB2BGR))\n",
    "    print ('Label_list\" {}'.format(label_list))\n",
    "    show_images(img_list, titles=label_list, bounding_boxes_list=bounding_boxes_list)\n",
    "    \n",
    "def show_random_images(xy_given_list, id_to_file_dict, count=4):\n",
    "    xy_rnd_idx_list = np.random.choice(len(xy_given_list), count, replace=False)\n",
    "    xy_rnd_list = [ xy_given_list[x] for x in xy_rnd_idx_list]\n",
    "    show_given_images(xy_rnd_list, id_to_file_dict)\n",
    "    return xy_rnd_list\n",
    "\n",
    "def resize_all(id_to_file_dict, prefix='data/processed', output_dir='resized_images', xsize=223, ysize=223, count=None):\n",
    "    output_dir = os.path.join(prefix, output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    ret_id_to_file_dict = {}\n",
    "    c = 0\n",
    "    for k, v in id_to_file_dict.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        try:\n",
    "            if os.path.isfile(v) is False:\n",
    "                print('Invalid file failed for {}'.format(v))\n",
    "                continue\n",
    "        except:\n",
    "            print('Invalid file failed for {}'.format(v))\n",
    "            raise\n",
    "        out_file = os.path.join(output_dir, os.path.basename(v))\n",
    "        \n",
    "        # If the file exists then \n",
    "        if os.path.isfile(out_file):\n",
    "            ret_id_to_file_dict[k] = out_file\n",
    "            continue\n",
    "            \n",
    "        if count is not None and c > count:\n",
    "            break\n",
    "        \n",
    "        resize_img = cv2.resize(cv2.imread(v, cv2.IMREAD_COLOR),(ysize, xsize))\n",
    "        out_file = os.path.join(output_dir, os.path.basename(v))\n",
    "        cv2.imwrite(out_file, resize_img)\n",
    "        \n",
    "        ret_id_to_file_dict[k] = out_file\n",
    "    return ret_id_to_file_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize and data prep code\n",
    "The following few cells help with saving resized images. Had to resize them to 96x96 to avoid out of memory issues. Have to fix these buy bumping up the memory on the box. Currently it only has 32GB. The full sized images are 28GB. Reducing them to 96x96 gets us to 803MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment if you want to resize the data again\n",
    "#X_resized_id_to_file_dict = resize_all(X_id_to_file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('Showing resized image with resized bounding box examples with bounding boxes')\n",
    "\n",
    "showen_list = show_random_images(xy_list, X_id_to_file_dict, count=4)\n",
    "\n",
    "print ('Showing resized but with original bounding box data image examples with bounding boxes')\n",
    "\n",
    "show_given_images(showen_list, X_resized_id_to_file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy_list, train_data_label_tuple, label_dict = process_raw_csv_input()\n",
    "xy_list, X_resized_id_to_file_dict = get_data_from_dir_recursive(xy_list, dir_input='resized_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Size of xy_list: {}\".format(len(xy_list)))\n",
    "print (\"xy_list[0]: {}, xy_list[0][0]: {}\".format(xy_list[0], xy_list[0][0]))\n",
    "print (\"Example file: {}\".format(X_resized_id_to_file_dict[xy_list[0][0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_image_list = show_random_images(xy_list, X_resized_id_to_file_dict, count=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the images and prep for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output data for retinanet to train the labels that work for us i.e\n",
    "# the object label and subject label\n",
    "def prepare_dataset_for_retinanet(xy_train_list, xy_test_list, id_to_file_dict, count=None):\n",
    "    def fix_boxes(box, xsize, ysize):\n",
    "        # input is Xmin, Xmax, Ymin, Ymax in %/faction of xsize and ysize\n",
    "        # retinanet needs x1,y1,x2,y2 in absolute of xsize and ysize\n",
    "        b = [ int(float(box[0])*xsize), int(float(box[2])*ysize), int(float(box[1])*xsize), \n",
    "             int(float(box[3])*ysize) ]\n",
    "        \"\"\"\n",
    "        Some of the bounding boxes does not satisfyin invariant constraint\n",
    "        x1 < x2 and y1 < y2. In such cases retinanet ignores those images.\n",
    "        Try to fix them here?. Fortunately they are only few such images.\n",
    "        if b[0] > b[2]:\n",
    "            tmp = b[2]\n",
    "            b[2] = b[0]\n",
    "            b[0] = tmp\n",
    "        if b[1] > b[3]:\n",
    "            tmp = b[3]\n",
    "            b[3] = b[1]\n",
    "            b[1] = tmp\n",
    "        \"\"\"\n",
    "        return b\n",
    "    \n",
    "    image_size_dict = {}\n",
    "    def get_dataset(xy_input_list):\n",
    "        dataset = {'features': [], 'bounding_box_with_label': []}\n",
    "        c = 0\n",
    "        for xy in xy_input_list:\n",
    "            # Add for first label\n",
    "            if count is not None and c > count:\n",
    "                break\n",
    "            c += 1\n",
    "            dataset['features'].append(id_to_file_dict[xy[0][0]])\n",
    "            if xy[0][0] in image_size_dict:\n",
    "                xsize, ysize = image_size_dict[xy[0][0]]\n",
    "            else:\n",
    "                img = cv2.imread(id_to_file_dict[xy[0][0]], cv2.IMREAD_COLOR)\n",
    "                xsize = img.shape[1]\n",
    "                ysize = img.shape[0]\n",
    "                image_size_dict[xy[0][0]] = (xsize, ysize)\n",
    "\n",
    "            dataset['bounding_box_with_label'].append(fix_boxes(xy[0][1][0:4], xsize, ysize) + [str(xy[1][0])])\n",
    "            # Add for second label       \n",
    "            dataset['features'].append(id_to_file_dict[xy[0][0]])\n",
    "            dataset['bounding_box_with_label'].append(\n",
    "            fix_boxes(xy[0][1][4:len(xy[0][1])], xsize, ysize) + [str(xy[1][1])])\n",
    "        return dataset\n",
    "\n",
    "    dataset_train = get_dataset(xy_input_list=xy_train_list)\n",
    "    dataset_test = get_dataset(xy_input_list=xy_test_list)\n",
    "    \n",
    "    return dataset_train, dataset_test\n",
    "\n",
    "def prepare_dataset_for_model(xy_train_list, xy_test_list, id_to_file_dict, count=None):\n",
    "    image_size_dict = {}\n",
    "    def get_dataset(xy_input_list):\n",
    "        dataset = {'features': [],  'labels_orig':[]}\n",
    "        c = 0\n",
    "        for xy in xy_input_list:\n",
    "            # Add for first label\n",
    "            if count is not None and c > count:\n",
    "                break\n",
    "            c += 1\n",
    "            dataset['features'].append(id_to_file_dict[xy[0][0]])\n",
    "            dataset['labels_orig'].append(xy[1])\n",
    "        return dataset\n",
    "    \n",
    "    dataset_train = get_dataset(xy_input_list=xy_train_list)\n",
    "    dataset_test = get_dataset(xy_input_list=xy_test_list)\n",
    "    \n",
    "    return dataset_train, dataset_test\n",
    "    \n",
    "# Write the CSV annotations\n",
    "def write_csv_annotations_for_retinanet(features, labels, \n",
    "                                        annotations_file='fullsize_train_annotations.csv'):\n",
    "    with open(annotations_file, mode='w', newline='', encoding='utf8') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for X, Y in zip(features, labels):\n",
    "            row = [X] + Y\n",
    "            writer.writerow(row)\n",
    "            \n",
    "def write_csv_classes_for_retinanet(train_data_label_tuple_input, classes_file='fullsize_classes.csv'):\n",
    "    labels_combined_dict = set()\n",
    "    # Make sure we have unique set of labels only the subject and object label. Should not contain\n",
    "    # relationship label.\n",
    "    for label in train_data_label_tuple[0].keys():\n",
    "        labels_combined_dict.add(label)\n",
    "    for label in train_data_label_tuple[1].keys():\n",
    "        labels_combined_dict.add(label)\n",
    "\n",
    "    with open(classes_file, mode='w', newline='', encoding='utf8') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        count = 0\n",
    "        for label in labels_combined_dict:\n",
    "            row = [str(label), count]\n",
    "            writer.writerow(row)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for retinatnet training.\n",
    "\n",
    "We take the full sized images label set given by the **challenge-2018-train-vrd.csv** which has 354122 entries and prepare it by breaking each entry into two examples i.e 1 per given bounding box and label.\n",
    "\n",
    "The following step will take some time since retinanet input needs bounding box in absolute dimensions rather than the perctange/fractional input given by OID data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy_list, train_data_label_tuple, label_dict = process_raw_csv_input(\n",
    "    prefix='data/raw', train_csv_fname = 'challenge-2018-train-vrd.csv', \n",
    "    labels_csv_fname_list = ['class-descriptions-boxable.csv', 'class-descriptions.csv'])\n",
    "xy_full_list, X_full_id_to_file_dict = get_data_from_dir_recursive(xy_list, dir_input='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break the input set to train and test set.\n",
    "\n",
    "Before generating the CSV input to Retinanet break the data up. So that we can use test set for evaulating the model and use the same test set for final model evaulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(xy_input_list, test_size=0.20, seed=42):\n",
    "    np.random.seed(seed=42)\n",
    "    np.random.shuffle(xy_input_list)\n",
    "    split_idx = int(round(float(len(xy_input_list)) * (1.0-test_size)))\n",
    "    return xy_input_list[0:split_idx], xy_input_list[split_idx:len(xy_input_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy_train_list, xy_test_list = train_test_split(xy_input_list=xy_full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Size of full set: {}, size of train set: {}, size of test set: {}, split %: {}'.format(\n",
    "    len(xy_full_list), len(xy_train_list), len(xy_test_list), round(100*float(len(xy_test_list))/len(xy_full_list), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for model without the bounding boxes from full size images \n",
    "#dataset_model_train, dataset_model_test = prepare_dataset_for_model(xy_train_list, xy_test_list, \n",
    "#                                                                    X_full_id_to_file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print an example from datatset to make sure it is sane.\n",
    "print ('Size of dataset: {}'.format(len(dataset_model_train['features'])))\n",
    "rnd_idx = np.random.choice(len(dataset_model_train['features'])-1, 2, replace=False)\n",
    "for i in rnd_idx:\n",
    "    print ('dataset[features][{}]: {}, dataset[labels_orig][{}]: {}'.format(i,\n",
    "        dataset_model_train['features'][i], i, dataset_model_train['labels_orig'][i]))\n",
    "    print ('dataset[features][{}]: {}, dataset[labels_orig]][{}]: {}'.format(i+1,\n",
    "        dataset_model_train['features'][i+1], i+1, dataset_model_train['labels_orig'][i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print an example from datatset to make sure it is sane.\n",
    "print ('Size of dataset: {}'.format(len(dataset_model_test['features'])))\n",
    "rnd_idx = np.random.choice(len(dataset_model_test['features'])-1, 2, replace=False)\n",
    "for i in rnd_idx:\n",
    "    print ('dataset[features][{}]: {}, dataset[labels_orig][{}]: {}'.format(i,\n",
    "        dataset_model_test['features'][i], i, dataset_model_test['labels_orig'][i]))\n",
    "    print ('dataset[features][{}]: {}, dataset[labels_orig][{}]: {}'.format(i+1,\n",
    "        dataset_model_test['features'][i+1], i+1, dataset_model_test['labels_orig'][i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for reinanet from full size images \n",
    "#dataset_train, dataset_test = prepare_dataset_for_retinanet(xy_train_list, xy_test_list, \n",
    "#                                                            X_full_id_to_file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print an example from datatset to make sure it is sane.\n",
    "print ('Size of dataset: {}'.format(len(dataset_train['features'])))\n",
    "rnd_idx = np.random.choice(len(dataset_train['features'])-1, 2, replace=False)\n",
    "for i in rnd_idx:\n",
    "    print ('dataset[features][{}]: {}, dataset[bounding_box_with_label][{}]: {}'.format(i,\n",
    "        dataset_train['features'][i], i, dataset_train['bounding_box_with_label'][i]))\n",
    "    print ('dataset[features][{}]: {}, dataset[bounding_box_with_label][{}]: {}'.format(i+1,\n",
    "        dataset_train['features'][i+1], i+1, dataset_train['bounding_box_with_label'][i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print an example from datatset to make sure it is sane.\n",
    "print ('Size of dataset: {}'.format(len(dataset_test['features'])))\n",
    "rnd_idx = np.random.choice(len(dataset_test['features'])-1, 2, replace=False)\n",
    "for i in rnd_idx:\n",
    "    print ('dataset[features][{}]: {}, dataset[bounding_box_with_label][{}]: {}'.format(i,\n",
    "        dataset_test['features'][i], i, dataset_test['bounding_box_with_label'][i]))\n",
    "    print ('dataset[features][{}]: {}, dataset[bounding_box_with_label][{}]: {}'.format(i+1,\n",
    "        dataset_test['features'][i+1], i+1, dataset_test['bounding_box_with_label'][i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the retinanet train data and train labels set as input for retinanet training from console.\n",
    "annotations_file='fullsize_train_annotations.csv'\n",
    "annotations_test_file='fullsize_test_annotations.csv'\n",
    "classes_file='fullsize_classes.csv'\n",
    "\n",
    "write_csv_annotations_for_retinanet(features=dataset_train['features'], \n",
    "                                    labels=dataset_train['bounding_box_with_label'], \n",
    "                                    annotations_file=annotations_file)\n",
    "write_csv_annotations_for_retinanet(features=dataset_test['features'], \n",
    "                                    labels=dataset_test['bounding_box_with_label'], \n",
    "                                    annotations_file=annotations_file)\n",
    "write_csv_classes_for_retinanet(train_data_label_tuple_input=train_data_label_tuple, classes_file=classes_file)\n",
    "\n",
    "# Store the model train and test data as pickle.\n",
    "import pickle\n",
    "train_model_pickle ='fullsize_train_model.p'\n",
    "test_model_pickle = 'fullsize_test_model.p'\n",
    "with open(train_model_pickle, \"wb\" ) as f:\n",
    "    pickle.dump(dataset_model_train, f)\n",
    "with open(test_model_pickle, \"wb\") as f:\n",
    "    pickle.dump(dataset_model_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following is the training script run to train on console\n",
    "\n",
    "```\n",
    "$ cd keras-retinanet/\n",
    "$ keras_retinanet/bin/train.py --imagenet-weights --freeze-backbone --no-evaluation --random-transform --epochs 50  --image-min-side 256 --image-max-side 427 --batch-size 1 csv /home/powell/work/scpd/vision_project/visual_relationship/fullsize_train_annotations.csv /home/powell/work/scpd/vision_project/visual_relationship/fullsize_classes.csv\n",
    "```\n",
    "\n",
    "Resnet50 is the default backbone used by retinnet and its weights are frozen at training time.\n",
    "\n",
    "*Total params: 37,627,657*\n",
    "*Trainable params: 14,066,505*\n",
    "*Non-trainable params: 23,561,152*\n",
    "\n",
    "The epoch snapshots will be stored in **keras-retinanet/snapshots** and tensorboard log data is in **keras-retinanet/logs**.\n",
    "\n",
    "The model output will be as follows:\n",
    "**keras-retinanet/snapshots/resnet50_csv_epoch_number.h5**, example **keras-retinanet/snapshots/resnet50_csv_01.h5** after 1 epoch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retinanet training epoch log\n",
    "\n",
    "```\n",
    "==================\n",
    "Total params: 37,627,657\n",
    "Trainable params: 14,066,505\n",
    "Non-trainable params: 23,561,152\n",
    "__________________________________________________________________________________________________\n",
    "None\n",
    "Epoch 1/50\n",
    "10000/10000 [==============================] - 539s 54ms/step - loss: 3.2986 - regression_loss: 2.4344 - classification_loss: 0.8642\n",
    "\n",
    "Epoch 00001: saving model to ./snapshots/resnet50_csv_01.h5\n",
    "Epoch 2/50\n",
    "10000/10000 [==============================] - 546s 55ms/step - loss: 2.9532 - regression_loss: 2.1671 - classification_loss: 0.7861\n",
    "\n",
    "Epoch 00002: saving model to ./snapshots/resnet50_csv_02.h5\n",
    "Epoch 3/50\n",
    " 2322/10000 [=====>........................] - ETA: 7:02 - loss: 2.9135 - regression_loss: 2.1233 - classification_loss: 0.7901keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:165: UserWarning: Image with id 28596 (shape (768, 1024, 3)) contains the following invalid boxes: [[  766.   707.  1594.   919.]\n",
    " [  766.   707.  1594.   919.]].\n",
    "  annotations['bboxes'][invalid_indices, :]\n",
    " 5743/10000 [================>.............] - ETA: 3:55 - loss: 2.8614 - regression_loss: 2.1055 - classification_loss: 0.7559keras_retinanet/bin/../../keras_retinanet/preprocessing/generator.py:165: UserWarning: Image with id 9600 (shape (1024, 768, 3)) contains the following invalid boxes: [[  449.   458.   785.  1016.]\n",
    " [  449.   458.   785.  1016.]].\n",
    "  annotations['bboxes'][invalid_indices, :]\n",
    "10000/10000 [==============================] - 551s 55ms/step - loss: 2.8223 - regression_loss: 2.0815 - classification_loss: 0.7408\n",
    "\n",
    "Epoch 00003: saving model to ./snapshots/resnet50_csv_03.h5\n",
    "Epoch 4/50\n",
    "10000/10000 [==============================] - 552s 55ms/step - loss: 2.7240 - regression_loss: 2.0191 - classification_loss: 0.7050\n",
    "\n",
    "Epoch 00004: saving model to ./snapshots/resnet50_csv_04.h5\n",
    "Epoch 5/50\n",
    "10000/10000 [==============================] - 540s 54ms/step - loss: 2.6735 - regression_loss: 1.9777 - classification_loss: 0.6958\n",
    "\n",
    "Epoch 00005: saving model to ./snapshots/resnet50_csv_05.h5\n",
    "Epoch 6/50\n",
    "10000/10000 [==============================] - 540s 54ms/step - loss: 2.6272 - regression_loss: 1.9394 - classification_loss: 0.6878\n",
    "\n",
    "Epoch 00006: saving model to ./snapshots/resnet50_csv_06.h5\n",
    "Epoch 7/50\n",
    "10000/10000 [==============================] - 541s 54ms/step - loss: 2.5968 - regression_loss: 1.9287 - classification_loss: 0.6681\n",
    "\n",
    "Epoch 00007: saving model to ./snapshots/resnet50_csv_07.h5\n",
    "Epoch 8/50\n",
    "10000/10000 [==============================] - 542s 54ms/step - loss: 2.5604 - regression_loss: 1.8956 - classification_loss: 0.6648\n",
    "\n",
    "Epoch 00008: saving model to ./snapshots/resnet50_csv_08.h5\n",
    "Epoch 9/50\n",
    "10000/10000 [==============================] - 539s 54ms/step - loss: 2.5373 - regression_loss: 1.8783 - classification_loss: 0.6590\n",
    "\n",
    "Epoch 00009: saving model to ./snapshots/resnet50_csv_09.h5\n",
    "Epoch 10/50\n",
    "10000/10000 [==============================] - 538s 54ms/step - loss: 2.4884 - regression_loss: 1.8454 - classification_loss: 0.6430\n",
    "\n",
    "Epoch 00010: saving model to ./snapshots/resnet50_csv_10.h5\n",
    "Epoch 11/50\n",
    "10000/10000 [==============================] - 545s 54ms/step - loss: 2.4869 - regression_loss: 1.8444 - classification_loss: 0.6424\n",
    "\n",
    "Epoch 00011: saving model to ./snapshots/resnet50_csv_11.h5\n",
    "Epoch 12/50\n",
    "10000/10000 [==============================] - 545s 55ms/step - loss: 2.4859 - regression_loss: 1.8377 - classification_loss: 0.6482\n",
    "\n",
    "Epoch 00012: saving model to ./snapshots/resnet50_csv_12.h5\n",
    "Epoch 13/50\n",
    "10000/10000 [==============================] - 536s 54ms/step - loss: 2.4415 - regression_loss: 1.8194 - classification_loss: 0.6222\n",
    "\n",
    "Epoch 00013: saving model to ./snapshots/resnet50_csv_13.h5\n",
    "Epoch 14/50\n",
    "10000/10000 [==============================] - 544s 54ms/step - loss: 2.4291 - regression_loss: 1.7997 - classification_loss: 0.6293\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Retinanet trained model and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "padding_conv1 (ZeroPadding2D)   (None, None, None, 3 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, None, None, 6 9408        padding_conv1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, None, None, 6 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, None, None, 6 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, None, None, 6 4096        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a_relu (Activation (None, None, None, 6 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding2a_branch2b (ZeroPadding (None, None, None, 6 0           res2a_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, None, None, 6 36864       padding2a_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b_relu (Activation (None, None, None, 6 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, None, None, 2 16384       res2a_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, None, None, 2 16384       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_relu (Activation)         (None, None, None, 2 0           res2a[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, None, None, 6 16384       res2a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a_relu (Activation (None, None, None, 6 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding2b_branch2b (ZeroPadding (None, None, None, 6 0           res2b_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, None, None, 6 36864       padding2b_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b_relu (Activation (None, None, None, 6 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, None, None, 2 16384       res2b_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              \n",
      "                                                                 res2a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2b_relu (Activation)         (None, None, None, 2 0           res2b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, None, None, 6 16384       res2b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a_relu (Activation (None, None, None, 6 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding2c_branch2b (ZeroPadding (None, None, None, 6 0           res2c_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, None, None, 6 36864       padding2c_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b_relu (Activation (None, None, None, 6 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, None, None, 2 16384       res2c_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              \n",
      "                                                                 res2b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2c_relu (Activation)         (None, None, None, 2 0           res2c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, None, None, 1 32768       res2c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a_relu (Activation (None, None, None, 1 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding3a_branch2b (ZeroPadding (None, None, None, 1 0           res3a_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, None, None, 1 147456      padding3a_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b_relu (Activation (None, None, None, 1 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, None, None, 5 65536       res3a_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, None, None, 5 131072      res2c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res3a_relu (Activation)         (None, None, None, 5 0           res3a[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, None, None, 1 65536       res3a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a_relu (Activation (None, None, None, 1 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding3b_branch2b (ZeroPadding (None, None, None, 1 0           res3b_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, None, None, 1 147456      padding3b_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b_relu (Activation (None, None, None, 1 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, None, None, 5 65536       res3b_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              \n",
      "                                                                 res3a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res3b_relu (Activation)         (None, None, None, 5 0           res3b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, None, None, 1 65536       res3b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a_relu (Activation (None, None, None, 1 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding3c_branch2b (ZeroPadding (None, None, None, 1 0           res3c_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, None, None, 1 147456      padding3c_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b_relu (Activation (None, None, None, 1 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, None, None, 5 65536       res3c_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              \n",
      "                                                                 res3b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res3c_relu (Activation)         (None, None, None, 5 0           res3c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, None, None, 1 65536       res3c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a_relu (Activation (None, None, None, 1 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding3d_branch2b (ZeroPadding (None, None, None, 1 0           res3d_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, None, None, 1 147456      padding3d_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b_relu (Activation (None, None, None, 1 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, None, None, 5 65536       res3d_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              \n",
      "                                                                 res3c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res3d_relu (Activation)         (None, None, None, 5 0           res3d[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, None, None, 2 131072      res3d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a_relu (Activation (None, None, None, 2 0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4a_branch2b (ZeroPadding (None, None, None, 2 0           res4a_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, None, None, 2 589824      padding4a_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b_relu (Activation (None, None, None, 2 0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, None, None, 1 262144      res4a_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, None, None, 1 524288      res3d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res4a_relu (Activation)         (None, None, None, 1 0           res4a[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, None, None, 2 262144      res4a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a_relu (Activation (None, None, None, 2 0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4b_branch2b (ZeroPadding (None, None, None, 2 0           res4b_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, None, None, 2 589824      padding4b_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b_relu (Activation (None, None, None, 2 0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, None, None, 1 262144      res4b_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              \n",
      "                                                                 res4a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4b_relu (Activation)         (None, None, None, 1 0           res4b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, None, None, 2 262144      res4b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a_relu (Activation (None, None, None, 2 0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4c_branch2b (ZeroPadding (None, None, None, 2 0           res4c_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, None, None, 2 589824      padding4c_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b_relu (Activation (None, None, None, 2 0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, None, None, 1 262144      res4c_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c (Add)                     (None, None, None, 1 0           bn4c_branch2c[0][0]              \n",
      "                                                                 res4b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4c_relu (Activation)         (None, None, None, 1 0           res4c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, None, None, 2 262144      res4c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a_relu (Activation (None, None, None, 2 0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4d_branch2b (ZeroPadding (None, None, None, 2 0           res4d_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, None, None, 2 589824      padding4d_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b_relu (Activation (None, None, None, 2 0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, None, None, 1 262144      res4d_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d (Add)                     (None, None, None, 1 0           bn4d_branch2c[0][0]              \n",
      "                                                                 res4c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4d_relu (Activation)         (None, None, None, 1 0           res4d[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, None, None, 2 262144      res4d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a_relu (Activation (None, None, None, 2 0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4e_branch2b (ZeroPadding (None, None, None, 2 0           res4e_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, None, None, 2 589824      padding4e_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b_relu (Activation (None, None, None, 2 0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, None, None, 1 262144      res4e_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e (Add)                     (None, None, None, 1 0           bn4e_branch2c[0][0]              \n",
      "                                                                 res4d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4e_relu (Activation)         (None, None, None, 1 0           res4e[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, None, None, 2 262144      res4e_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a_relu (Activation (None, None, None, 2 0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding4f_branch2b (ZeroPadding (None, None, None, 2 0           res4f_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, None, None, 2 589824      padding4f_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b_relu (Activation (None, None, None, 2 0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, None, None, 1 262144      res4f_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f (Add)                     (None, None, None, 1 0           bn4f_branch2c[0][0]              \n",
      "                                                                 res4e_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4f_relu (Activation)         (None, None, None, 1 0           res4f[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, None, None, 5 524288      res4f_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a_relu (Activation (None, None, None, 5 0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding5a_branch2b (ZeroPadding (None, None, None, 5 0           res5a_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5a_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b_relu (Activation (None, None, None, 5 0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, None, None, 2 1048576     res5a_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, None, None, 2 2097152     res4f_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a (Add)                     (None, None, None, 2 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res5a_relu (Activation)         (None, None, None, 2 0           res5a[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, None, None, 5 1048576     res5a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a_relu (Activation (None, None, None, 5 0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding5b_branch2b (ZeroPadding (None, None, None, 5 0           res5b_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5b_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b_relu (Activation (None, None, None, 5 0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, None, None, 2 1048576     res5b_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b (Add)                     (None, None, None, 2 0           bn5b_branch2c[0][0]              \n",
      "                                                                 res5a_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res5b_relu (Activation)         (None, None, None, 2 0           res5b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, None, None, 5 1048576     res5b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a_relu (Activation (None, None, None, 5 0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "padding5c_branch2b (ZeroPadding (None, None, None, 5 0           res5c_branch2a_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5c_branch2b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b_relu (Activation (None, None, None, 5 0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, None, None, 2 1048576     res5c_branch2b_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c (Add)                     (None, None, None, 2 0           bn5c_branch2c[0][0]              \n",
      "                                                                 res5b_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res5c_relu (Activation)         (None, None, None, 2 0           res5c[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "C5_reduced (Conv2D)             (None, None, None, 2 524544      res5c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]                 \n",
      "                                                                 res4f_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C4_reduced (Conv2D)             (None, None, None, 2 262400      res4f_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]               \n",
      "                                                                 C4_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]                  \n",
      "                                                                 res3d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C3_reduced (Conv2D)             (None, None, None, 2 131328      res3d_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P6 (Conv2D)                     (None, None, None, 2 4718848     res5c_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]               \n",
      "                                                                 C3_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P5 (Conv2D)                     (None, None, None, 2 590080      C5_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "anchors_0 (Anchors)             (None, None, 4)      0           P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "anchors_1 (Anchors)             (None, None, 4)      0           P4[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "anchors_2 (Anchors)             (None, None, 4)      0           P5[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "anchors_3 (Anchors)             (None, None, 4)      0           P6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "anchors_4 (Anchors)             (None, None, 4)      0           P7[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]                         \n",
      "                                                                 P4[0][0]                         \n",
      "                                                                 P5[0][0]                         \n",
      "                                                                 P6[0][0]                         \n",
      "                                                                 P7[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "anchors (Concatenate)           (None, None, 4)      0           anchors_0[0][0]                  \n",
      "                                                                 anchors_1[0][0]                  \n",
      "                                                                 anchors_2[0][0]                  \n",
      "                                                                 anchors_3[0][0]                  \n",
      "                                                                 anchors_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]        \n",
      "                                                                 regression_submodel[2][0]        \n",
      "                                                                 regression_submodel[3][0]        \n",
      "                                                                 regression_submodel[4][0]        \n",
      "                                                                 regression_submodel[5][0]        \n",
      "__________________________________________________________________________________________________\n",
      "boxes (RegressBoxes)            (None, None, 4)      0           anchors[0][0]                    \n",
      "                                                                 regression[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "classification_submodel (Model) (None, None, 61)     3625765     P3[0][0]                         \n",
      "                                                                 P4[0][0]                         \n",
      "                                                                 P5[0][0]                         \n",
      "                                                                 P6[0][0]                         \n",
      "                                                                 P7[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "clipped_boxes (ClipBoxes)       (None, None, 4)      0           input_1[0][0]                    \n",
      "                                                                 boxes[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "classification (Concatenate)    (None, None, 61)     0           classification_submodel[1][0]    \n",
      "                                                                 classification_submodel[2][0]    \n",
      "                                                                 classification_submodel[3][0]    \n",
      "                                                                 classification_submodel[4][0]    \n",
      "                                                                 classification_submodel[5][0]    \n",
      "__________________________________________________________________________________________________\n",
      "filtered_detections (FilterDete [(None, 300, 4), (No 0           clipped_boxes[0][0]              \n",
      "                                                                 classification[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 37,627,657\n",
      "Trainable params: 14,066,505\n",
      "Non-trainable params: 23,561,152\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Retinanet model testing\n",
    "# adjust this to point to your downloaded/trained model\n",
    "# models can be downloaded here: https://github.com/fizyr/keras-retinanet/releases\n",
    "model_path = os.path.join('retinanet_snapshot', 'resnet50_csv_12.h5')\n",
    "\n",
    "# load retinanet model\n",
    "retinanet_model = models.load_model(model_path, backbone_name='resnet50')\n",
    "\n",
    "# if the model is not converted to an inference model, use the line below\n",
    "# see: https://github.com/fizyr/keras-retinanet#converting-a-training-model-to-inference-model\n",
    "retinanet_model_infer = models.convert_model(retinanet_model)\n",
    "\n",
    "print(retinanet_model_infer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores = retinanet_model_infer.evaluate(np.array(X_test), np.array(y_test))\n",
    "print('\\n{}: {}'.format(retinanet_model_infer.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the labels\n",
    "lable_tag_to_name = process_labels_from_csv_input()\n",
    "def load_labels(label_tag_to_name, labels_file=\"fullsize_classes.csv\"):\n",
    "    labels_to_names = {}\n",
    "    # Process the labels info files and create a key value pair\n",
    "    with open(labels_file, encoding='utf8') as f:\n",
    "        rows = csv.reader(f)\n",
    "        for row in rows:\n",
    "            labels_to_names[int(row[1])]=label_tag_to_name[row[0]]\n",
    "    return labels_to_names\n",
    "\n",
    "labels_to_names = load_labels(lable_tag_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'Camera'), (1, 'Briefcase'), (2, 'Rugby ball'), (3, 'Chopsticks'), (4, 'Bottle'), (5, 'Knife'), (6, 'Beer'), (7, 'Boy'), (8, 'Pretzel'), (9, 'Table tennis racket'), (10, 'Piano'), (11, 'Snake'), (12, 'Van'), (13, 'Sofa bed'), (14, 'Tennis ball'), (15, 'Table'), (16, 'Bed'), (17, 'Dog'), (18, 'Coffee cup'), (19, 'Textile'), (20, 'Microwave oven'), (21, 'Wine glass'), (22, 'Ski'), (23, 'Elephant'), (24, 'Violin'), (25, 'Plastic'), (26, 'Suitcase'), (27, 'Flute'), (28, 'Man'), (29, 'Mug'), (30, 'Mobile phone'), (31, 'Handbag'), (32, 'Bench'), (33, 'Coffee table'), (34, 'Oven'), (35, 'Girl'), (36, 'Wood'), (37, 'Fork'), (38, 'Tennis racket'), (39, 'Bicycle'), (40, 'Monkey'), (41, 'Cat'), (42, 'Desk'), (43, 'Chair'), (44, 'Football'), (45, 'Leather'), (46, 'Woman'), (47, 'Car'), (48, 'Drum'), (49, 'Backpack'), (50, 'Motorcycle'), (51, 'Snowboard'), (52, 'Microphone'), (53, 'Taxi'), (54, 'Horse'), (55, 'Surfboard'), (56, 'Guitar'), (57, 'Hamster'), (58, 'Spoon'), (59, 'Racket'), (60, 'Dolphin')]\n"
     ]
    }
   ],
   "source": [
    "print (sorted(labels_to_names.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = read_image_bgr(X_resized_test_id_to_file_dict[xy_test_list[15][0][0]])\n",
    "\n",
    "# copy to draw on\n",
    "draw = image.copy()\n",
    "draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# preprocess image for network\n",
    "image = preprocess_image(image)\n",
    "image, scale = resize_image(image, min_side=246, max_side=427)\n",
    "\n",
    "print ('Scale: {}'.format(scale))\n",
    "# process image\n",
    "start = time.time()\n",
    "boxes, scores, labels = reinanet_model_infer.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "print(\"processing time: \", time.time() - start)\n",
    "#print ('Boxes: {}, scores: {}, labels: {}'.format(boxes, scores, labels))\n",
    "\n",
    "# correct for image scale\n",
    "boxes /= scale\n",
    "\n",
    "# visualize detections\n",
    "top_k=10\n",
    "box_score_label_list = [ x for x in zip(boxes[0], scores[0], labels[0]) ]\n",
    "box_score_label_list = sorted(box_score_label_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in range(0, top_k):\n",
    "    box, score, label = box_score_label_list[i]\n",
    "    # scores are sorted so we can break\n",
    "        \n",
    "    color = label_color(label)\n",
    "    \n",
    "    b = box.astype(int)\n",
    "    draw_box(draw, b, color=color)\n",
    "    \n",
    "    label_name = label\n",
    "    #print (label_name)\n",
    "    if label_name in labels_to_names:\n",
    "        label_name = labels_to_names[label]\n",
    "    print ('Label: {}, Score: {}'.format(label_name, score))\n",
    "    caption = \"{} {:.3f}\".format(label_name, score)\n",
    "    draw_caption(draw, b, caption)\n",
    "    \n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axis('off')\n",
    "plt.imshow(draw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_model_pickle ='fullsize_train_model.p'\n",
    "test_model_pickle = 'fullsize_test_model.p'\n",
    "with open(train_model_pickle, \"rb\" ) as f:\n",
    "    dataset_model_train = pickle.load(f)\n",
    "with open(test_model_pickle, \"rb\") as f:\n",
    "    dataset_model_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the images and prep for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image, compute_resize_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log_progress import log_progress\n",
    "\n",
    "image_processed_dict = {}\n",
    "def process_data_sets_for_model(dataset_raw, min_side=256, max_side=427, mode='caffe', count = None):\n",
    "    dataset_processed = {'features': [], 'scale': [], 'labels': [], 'labels_orig': []}\n",
    "    c = 0\n",
    "    for x, y in log_progress(iter(zip(dataset_raw['features'], dataset_raw['labels_orig'])), every=10):\n",
    "        if c >= count:\n",
    "            break\n",
    "        c += 1\n",
    "        img = None\n",
    "        if x in image_processed_dict:\n",
    "            img = image_processed_dict[x]\n",
    "        else:\n",
    "            img = read_image_bgr(x)\n",
    "            img, scale = resize_image(img, min_side=min_side, max_side=max_side)\n",
    "            img = preprocess_image(img, mode=mode)\n",
    "            image_processed_dict[x] = img\n",
    "        dataset_processed['features'].append(img)\n",
    "        dataset_processed['scale'].append(scale)\n",
    "        dataset_processed['labels_orig'].append(y)\n",
    "    return dataset_processed\n",
    "    \n",
    "dataset_train_processed = process_data_sets_for_model(dataset_model_train, count=5000)\n",
    "dataset_test_processed = process_data_sets_for_model(dataset_model_test, count=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Multi Label Binarizer to encode the labels for the images.\n",
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set size: 5000\n",
      "Test data set size: 1000\n",
      "Before Multi-Encoding eature shape: (256, 387, 3), label: ('/m/04bcr3', '/m/083vt', 'is')\n",
      "After Multi-Encoding eature shape: (256, 387, 3), label: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "MLB classes size 61, classes: ['/m/01226z' '/m/01599' '/m/01940j' '/m/0199g' '/m/019w40' '/m/01_5g'\n",
      " '/m/01bl7v' '/m/01mzpv' '/m/01s55n' '/m/01y9k5' '/m/01yrx' '/m/026t6'\n",
      " '/m/02jvh9' '/m/02p5f1q' '/m/0342h' '/m/03bt1vf' '/m/03k3r' '/m/03m3pdh'\n",
      " '/m/03ssj5' '/m/04_sv' '/m/04bcr3' '/m/04ctx' '/m/04dr76w' '/m/04lbp'\n",
      " '/m/04yx4' '/m/050k8' '/m/0584n8' '/m/05_5p_0' '/m/05r5c' '/m/05r655'\n",
      " '/m/05z87' '/m/071p9' '/m/078jl' '/m/078n6m' '/m/07y_7' '/m/080hkjn'\n",
      " '/m/083vt' '/m/09tvcd' '/m/0bt9lr' '/m/0cmx8' '/m/0cvnqh' '/m/0dnr7'\n",
      " '/m/0dt3t' '/m/0dv5r' '/m/0dv9c' '/m/0h2r6' '/m/0h8my_4' '/m/0hg7b'\n",
      " '/m/0k4j' '/m/0l14j_' '/m/0pg52' '/m/0wdt60w' 'at' 'hits' 'holds'\n",
      " 'inside_of' 'interacts_with' 'is' 'on' 'plays' 'wears']\n"
     ]
    }
   ],
   "source": [
    "print ('Training data set size: {}'.format(len(dataset_train_processed['features'])))\n",
    "print ('Test data set size: {}'.format(len(dataset_test_processed['features'])))  \n",
    "\n",
    "print ('Before Multi-Encoding eature shape: {}, label: {}'.format(dataset_train_processed['features'][0].shape, dataset_train_processed['labels_orig'][0]))\n",
    "\n",
    "# Multi-label encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb_fit = mlb.fit(dataset_train_processed['labels_orig'] + dataset_test_processed['labels_orig'])\n",
    "                  \n",
    "dataset_train_processed['labels'] = mlb.transform(dataset_train_processed['labels_orig'])\n",
    "dataset_test_processed['labels'] = mlb.transform(dataset_test_processed['labels_orig'])\n",
    "                  \n",
    "print ('After Multi-Encoding eature shape: {}, label: {}'.format(dataset_train_processed['features'][0].shape, dataset_train_processed['labels'][0]))\n",
    "print('MLB classes size {}, classes: {}'.format(len(mlb.classes_), mlb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Relation model.\n",
    "We will use the trained Retinanet model which will output bounding boxes proposals and labels for them. We will add few more layers on top of the model and add add logit at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet.utils.model import freeze\n",
    "def RelationshipModel(input_model, output_size):\n",
    "    \"\"\"\n",
    "    Model is regular CNN without Pooling for some or all layers\n",
    "    \"\"\"\n",
    "    # Taken from https://github.com/fizyr/keras-retinanet\n",
    "    assert(all(output in input_model.output_names for output in ['regression', 'classification'])), \\\n",
    "        \"Input is not a training model (no 'regression' and 'classification' outputs were found, outputs are: {}).\".format(input_model.output_names)\n",
    "\n",
    "    input_model.layers.pop()\n",
    "    input_model = freeze(input_model)   \n",
    "        \n",
    "    model = Sequential()\n",
    "    # Layer 1\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # Layer 2\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # Output Layer\n",
    "    model.add(Dense(output_size, activation='sigmoid'))\n",
    "\n",
    "    return keras.models.Model(inputs=input_model.inputs, outputs=model(input_model.output), name=\"triplet_relation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and Optimization\n",
    "\n",
    "We use the **Focal** loss function and **Adam** optimizer. Batch size is set to 64 and number of default epochs are 32.\n",
    "**We note that more investigation is need to find out a correct loss function and an accuracy measure to help SGD find the optimal weights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras_retinanet import losses\n",
    "\n",
    "def train_model(model, x_train, y_train, learn_rate=0.001, epochs=32, batch_size=64, verbose=1):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    Using as loss function and Adam optimizer, default learning rate is .001\n",
    "    \"\"\"\n",
    "    model.compile(loss=losses.focal, \n",
    "        optimizer=keras.optimizers.Adam(lr=learn_rate, decay=learn_rate / epochs), metrics=['accuracy'])\n",
    "    return model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "        verbose=verbose, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retinanet model input: (?, ?, ?, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7384c6f4e956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Retinanet model input: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretinanet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRelationshipModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretinanet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-b2de8c8b2162>\u001b[0m in \u001b[0;36mRelationshipModel\u001b[0;34m(input_model, output_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"triplet_relation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    216\u001b[0m             x = Input(batch_shape=batch_shape,\n\u001b[1;32m    217\u001b[0m                       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                       name=self.name + '_input')\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                          name=self.name)\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0mevaluated\u001b[0m \u001b[0mdirectly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m   \"\"\"\n\u001b[0;32m-> 1548\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_placeholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   2092\u001b[0m   \"\"\"\n\u001b[1;32m   2093\u001b[0m   result = _op_def_lib.apply_op(\"Placeholder\", dtype=dtype, shape=shape,\n\u001b[0;32m-> 2094\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m   2095\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    703\u001b[0m               [_MakeType(x, attr_def) for x in value])\n\u001b[1;32m    704\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m           \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MakeShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"list(shape)\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m           attr_value.list.shape.extend(\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_MakeShape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    437\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    437\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENTER/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n\u001b[1;32m     34\u001b[0m           self._value != value):\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "print ('Retinanet model input: {}'.format(retinanet_model.input.shape))\n",
    "model = RelationshipModel(retinanet_model, output_size=len(mlb.classes_))\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "epochs = 4\n",
    "batch_size = 8\n",
    "model_history = train_model(model, np.array(X_train), np.array(y_train), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "Save the model that has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the weights.\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Save the model architecture.\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "    \n",
    "# Save the MLB labels for later use.\n",
    "with open(\"mlb_labels.p\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(mlb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, epochs), model_history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, epochs), model_history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, epochs), model_history.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, epochs), model_history.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(\"model_training_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalid accuracy score of test set.\n",
    "Due to multi-label classification problem the accuracy is artificially high due the fact that many labels are correctly classified as False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(np.array(dataset_test_processed['features']), np.array(dataset_test_processed['labels']))\n",
    "print('\\n{}: {}'.format(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(np.expand_dims(X_train[0], axis=0))\n",
    "print('Pred : {}'.format((pred * 100).flatten().tolist()))\n",
    "print('Truth: {}'.format(y_train[0].flatten().tolist()))\n",
    "\n",
    "def check_pred(pred, ground):\n",
    "    pred = pred.flatten().tolist()\n",
    "    ground = ground.flatten().tolist()\n",
    "    max_check = float('-inf')\n",
    "    \n",
    "    # Get the maximum in pred not corresponding to\n",
    "    # categories set to true in ground.\n",
    "    l = [] # Save the ground indices where True exists.\n",
    "    for i in range(0, len(ground)):\n",
    "        if ground[i] == 0:\n",
    "            max_check = max(max_check, pred[i])\n",
    "        else:\n",
    "            l.append(i)\n",
    "    \n",
    "    # Check that all other elements that do not\n",
    "    # correspond to prediction do not exceed the minumum above.\n",
    "    count = 0\n",
    "    for i in l:\n",
    "        if max_check < pred[i]:\n",
    "            count +=1\n",
    "    \n",
    "    # Did we get all, if not how many?.\n",
    "    return count == len(l), count\n",
    "\n",
    "print ('check_pred: {}'.format(check_pred(pred, y_train[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set error calculation\n",
    "Iterate through every test set example get the prediction using the model and check with the available label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_calc_helper(feature_set, label_set):\n",
    "    accurate_prediction_count_dict = {0: [], 1: [], 2: [], 3:[]}\n",
    "    for i in range(0, len(feature_set)):\n",
    "        pred = model.predict(np.expand_dims(feature_set[i], axis=0))\n",
    "        success, success_count = check_pred(pred, label_set[i])\n",
    "        if success:\n",
    "            assert success_count == 3\n",
    "        accurate_prediction_count_dict[success_count].append((i, pred))\n",
    "\n",
    "    acc_percentages = {}\n",
    "    for k, v in accurate_prediction_count_dict.items():\n",
    "        acc_percentages[k] = str(round(len(v)*100.0/len(feature_set), 2))+'%'\n",
    "        \n",
    "    return accurate_prediction_count, acc_percentages\n",
    "\n",
    "accurate_prediction_count, acc_percentages = accuracy_calc_helper(X_test, y_test)\n",
    "\n",
    "print('Total Test samples: {}, Accuracy percentage map: {}'.format(len(X_test), acc_percentages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set error calculation\n",
    "Let us check if network is actually working for the training set.\n",
    "We showcase here the pitfall of loss function used in training which gives a wrong idea of accuracy at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accurate_prediction_count, acc_percentages = accuracy_calc_helper(X_train, y_train)\n",
    "print('Total Train samples: {}, Accuracy percentage map: {}'.format(len(X_train), acc_percentages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Things to do.\n",
    "\n",
    "1. Investigate a better loss function and different network architectures. \n",
    "2. Incorporate advances in semantic segmentation networks by using the bounding box data. \n",
    "3. Investigate which pre trained network to use to help with object detection part of the network.\n",
    "4. Investigate the feasibility of mining language data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {
    "0df598a9b33d4e9283b716ed0f03f25d": {
     "views": []
    },
    "101fd721a8bd4f3985b3b17a2452f47c": {
     "views": []
    },
    "25b79e02412a4f6285a5bc399b4caa5c": {
     "views": []
    },
    "35b0565f372a49a2a25dedfbf119b020": {
     "views": []
    },
    "3eeb12abd0464630b0e565def9daf93a": {
     "views": []
    },
    "40176b5f76cb4ea29f07632b385dfac7": {
     "views": []
    },
    "52d59e97dcf5480e8359b1d482d0b585": {
     "views": []
    },
    "539576cdc99448c4accab50b8e4a5eb4": {
     "views": []
    },
    "5749d1b711a6426fb71904651633856e": {
     "views": []
    },
    "5996555f520c4428b1929d4b2dcfe271": {
     "views": [
      {
       "cell_index": 37
      }
     ]
    },
    "5ccfa36ecedf458b966e700c78fa2881": {
     "views": []
    },
    "7e58bf1e9e7b4b4c848be05f5b4dffe4": {
     "views": []
    },
    "8a8c5b3158154c738e8a7211a1dba3a5": {
     "views": [
      {
       "cell_index": 37
      }
     ]
    },
    "9425bce2e3f44404bc7ba2457b68ae2b": {
     "views": []
    },
    "9d453caeab76465ba11aef3c8cfb35ad": {
     "views": []
    },
    "9df3d5e9db6a4e198bb5e855164eda17": {
     "views": []
    },
    "a47b48ff70bd4c38859c069411a0d0c8": {
     "views": []
    },
    "b066286d6306443da83b1b235073ae4c": {
     "views": []
    },
    "b7da08e1a45f418abca446e03a5bb04f": {
     "views": [
      {
       "cell_index": 37
      }
     ]
    },
    "be6a061daecc47f79b027b5a26d610ab": {
     "views": []
    },
    "ca8cb9df48b64672a3b50db473ff748f": {
     "views": []
    },
    "e5ce90b023ec4626a193fb18d9f38b38": {
     "views": []
    },
    "ec18d68580bb4909afb3fe484bfe7d9c": {
     "views": []
    },
    "f25a1c769ad843e4b90e26f49965c9b2": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
